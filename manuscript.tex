% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa7}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{multigroup confirmatory factor analysis, measurement invariance, visualization, effect size}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={visualizemi: Visualization, Effect Size, and Replication of Measurement Invariance for Registered Reports},
  pdfauthor={Erin M. Buchanan1},
  pdflang={en-EN},
  pdfkeywords={multigroup confirmatory factor analysis, measurement invariance, visualization, effect size},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{visualizemi: Visualization, Effect Size, and Replication of Measurement Invariance for Registered Reports}
\author{Erin M. Buchanan\textsuperscript{1}}
\date{}


\shorttitle{VISUAL MGCFA}

\authornote{

Thank you to K.D. Valentine and Chelsea Parlett-Pelleriti for feedback on some ugly graphs.

Correspondence concerning this article should be addressed to Erin M. Buchanan, 326 Market St., Harrisburg, PA, USA. E-mail: \href{mailto:ebuchanan@harrisburgu.edu}{\nolinkurl{ebuchanan@harrisburgu.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Harrisburg University of Science and Technology}

\abstract{%
Latent variable modeling as a lens for psychometric theory is a popular tool for social scientists to examine measurement of constructs (Beaujean, 2014). Journals such as \emph{Assessment} regularly publish articles supporting measures of latent constructs wherein a measurement model is established. Confirmatory factor analysis can be used to investigate the replicability and generalizability of the measurement model in new samples, while multi-group confirmatory factor analysis is used to examine the measurement model across groups within samples (Brown, 2015). With the rise of the replication crisis and ``psychology's renaissance'' (Nelson et al., 2018), interest in divergence in measurement has increased, often focused on small parameter differences within the latent model. This manuscript presents \texttt{visualizemi}, an \emph{R} package that provides functionality to calculate multigroup models, partial invariance, visualizations for (non)-invariance, effect sizes for models and parameters, and potential replication rates compared to random models. Readers will learn how to interpret the impact and size of the proposed non-invariance in models with a focus on potential replication and how to plan for registered reports.
}



\begin{document}
\maketitle

Psychological assessments play a critical role in our ability to measure and analyze constructs to support theories and experimental hypotheses. Defining and creating assessments to validly and reliability measure constructs is often difficult because phenomenon, such as anxiety, are often not directly observable. Instead, we use surveys and questionnaires to indirectly assess the underlying construct (DeVellis \& Thorpe, 2022). Latent variable modeling (i.e., structural equation modeling) is a popular tool for the validation of developed survey instruments to verify scale dimensionality, structure, and model fit. A simple search for scale development reveals thousands of articles in psychology that examine new and previously published work, thus, illustrating the interest in both measurement and the use of validation techniques. Unfortunately, except in specialty journals, much of the validity evidence and/or development for measures used in empirical studies is not reported within the journal article (Barry et al., 2014; Weidman et al., 2017). Without this information, it is difficult to interpret individual study conclusions, as validity information allows for judgment of usefulness of the measured values (Flake \& Fried, 2020). Further, the current focus on replication (Makel et al., 2012; Makel \& Plucker, 2014; Zwaan et al., 2018), reproducibility (Nelson et al., 2018), and the credibility of our results (Vazire et al., 2022) has demonstrated questionable measurement practices - decisions that researchers make like survey selection and scoring that impact the results of the study (Flake \& Fried, 2020). Transparent reporting of the use and creation of scales can improve both interpretation and reproducibility when using surveys developed to measure latent constructs (Shadish et al., 2001).

A secondary concern for developed measures is the potential for differential responding and assessment within target populations. For example, Trent et al. (2013) examined for potential variability in the Revised Child Anxiety and Depression Scale in White and Black youths (Chorpita et al., 2000). They found that the scale mostly functioned the same for both White and Black individuals but differences in averages on individual items could potentially affect the scoring and interpretation of the scale results. This comparison of sub-populations is the test of measurement invariance (Meredith, 1993). Invariance or equivalence implies that the scale operates in the same fashion for each sub-group, and thus, differences in the final latent variable scores can interpreted as differences in populations. Non-invariance suggests that individuals respond or interpret items differently, and thus, differences in scores may represent different scores on the latent variable in the population or differences in measurement. Non-invariant measurement may lead to misleading results when making group comparisons, and assessing invariance has become a popular technique in scale development (Van De Schoot et al., 2015).

Measurement invariance has been explored and implemented for the last fifty years (Jöreskog, 1971; Sörbom, 1978) and implemented in the most popular structural equation modeling programs (Boker et al., 2011; Jöreskog \& Sörbom, 2001; Rosseel, 2012). Byrne et al. (1989) extended the ideas of multi-group testing by suggesting partial invariance (followed by Meredith, 1993). Partial invariance occurs when non-invariance is found but can be attributed to only a few parameter estimate differences between groups (i.e., items 1 and 2 have different factor loadings but all others are the same). This testing provided an advantage to understand where the potential non-invariance may occur for further study and interpretation guidelines. As the field pushes back against favoring cutoff criteria and rules of thumb (Marsh et al., 2004; Putnick \& Bornstein, 2016), an effect size measure for translating ``how much'' non-invariance was developed \(d_{MACS}\) (Nye \& Drasgow, 2011). This effect size examines the differences in observed variables between the two groups for both the factor loading and the item intercept; thus, any differences in either or both will increase the effect size for non-invariance (Stark et al., 2006).

With \(d_{MACS}\) and measurement invariance testing, researchers can begin to quantify how and where their construct measurement may vary between groups. Yet, given the large number of studies that show non-invariance, it is clear that equivalence can be hard to meet. It is difficult to know if non-invariance occurs because of random sampling error, true population differences, or differences in replication and reproducibility of the construct in a new sample. The field of psychology is increasingly interested in pre-registration (i.e., registering plans for a study before data collection, Nosek et al., 2018) and the promotion of transparency in study design, implementation, and analysis (Mayo-Wilson et al., 2021), in addition to supporting replication studies (Zwaan et al., 2018). Registered (replication) reports provide an advantageous avenue for the pre-registration of measurement tests, as they allow a researcher the ability to have their study accepted in principle, regardless of the results of a test of construct validity, reliability, or measurement invariance (Hobson, 2019; Nosek \& Lakens, 2014). However, there are few tools that can provide effect size measures for models, individual parameters, or visualization for researchers to plan for future studies. \(d_{MACS}\) provides the opportunity to begin to think about the smallest effect size of interest or the smallest meaningful effect size for measurement invariance and replication (Anvari \& Lakens, 2021; i.e., two studies with overlapping confidence intervals ``replicate,'' even if the test of measurement invariance does not, Lakens, 2017). As mentioned, \(d_{MACS}\) has only really been explored for a combined intercept and loadings, and while useful, does not necessarily allow a researcher to pinpoint specific issues within an observed variable.

Therefore, purpose of this manuscript is describe an \emph{R} package, \texttt{visualizemi}, that provides functionality to calculate multi-group confirmatory factor analysis, partial invariance tests, visualizations of the size of non-invariance, and potential effect sizes for overall models and individual parameters. No known visualization techniques have been proposed for measurement invariance. By creating panel visualizations, we can supplement a researchers ability to judge the strength of the non-invariance differences and effect size for each item. The proposed effect sizes demonstrate the likelihood of replication with a similar sample as compared to a randomly assigned group model, thus, illustrating what type of measurement one might expect to find, and how different that is from random chance. Within this technique, the individual parameter effect sizes can calculated: both the group differences within a model as compared to random and the likelihood of a parameter replication compared to random groups. Coupled with other indicators (i.e., fit indices differences, \(d_{MACS}\)), we can move toward a better understanding of how much measurement non-invariance is meaningful. This tutorial and package will help researchers plan future studies and aid in the ability to estimate a smallest effect of interest for measurement invariance studies, rather than relying on fit indices and rules of thumb alone.

By the end of this tutorial manuscript, readers will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Learn how to use \emph{visualizemi} to analyze multi-group confirmatory factor analysis, examine partial invariance, and create visualizations of parameters.
\item
  Learn how to estimate the potential replication of multi-group models and their parameters using bootstrapping compared to a random group model.
\item
  Be able to calculate and interpret effect sizes for model and parameter replication, as well as parameter group differences.
\item
  Understand the impact of measurement variability on replication and generalizability.
\end{enumerate}

The tutorial will start with simulated data based on known effect sizes using \(d_{MACS}\) and demonstrating the package functions for 1) running the multigroup analysis, 2) running a partial invariance analysis, 3) plotting the partial invariance, 4) estimating replication and effect sizes at the model level, and 5) estimating replication and effect sizes at the parameter level. Last, data from Aiena et al. (2014) examining the measurement invariance of the RS-14 (Wagnild, 2009) will be used to demonstrate the application of the package on real data. The \emph{visualizemi} package vignette includes an additional tutorial walk through.

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{design-and-analysis}{%
\subsection{Design and Analysis}\label{design-and-analysis}}

Data was simulated using the \texttt{simulateData} function in the \emph{R} package \emph{lavaan} (Rosseel, 2012) assuming multivariate normality using a \(\mu\) of 0 and \(\sigma\) of 1 for the data. This function allows you to write \emph{lavaan} syntax for your model with estimated values to generate data for observed variables (see supplemental for examples). The data included two groups of individuals (``Group 1'', ``Group 2'') for a multi-group confirmatory factor analysis (\(n_{group}\) = 250, \emph{N} = 500). The latent variables were assumed to be continuous normal (the package functions do not require this assumption). The model consisted of five observed items predicted by one latent variable (\texttt{lv\ =\textasciitilde{}\ q1\ +\ q2\ +\ q3\ +\ q4\ +\ q5}); however, the demonstration in this manuscript extends to multiple latent variables and other combinations of observed variables. Each item was assumed to be related to the latent variable with loadings approximately equal to .40 to .80, except when cases of non-invariance on the loadings was simulated.

The Brown (2015) steps of testing measurement invariance are demonstrated in this manuscript for illustration purposes, but in line with Stark et al. (2006) suggestions, the visualizations show the impact of loadings and intercepts together. A convenience function \texttt{mgcfa} is used for these steps or other measurement invariance test orders and combinations. Fit indices for the steps for multi-group models are presented in the appendix for comparison of cutoff rules of thumb (Cheung \& Rensvold, 2002) to effect sizes and visualizations presented in this manuscript. Fit indices include Akaike Information Criterion (AIC, Akaike, 1998), Bayesian Information Criterion (BIC, Schwarz, 1978), Comparative Fit Index (CFI, Bentler, 1990), Tucker Lewis Index (TLI, Tucker \& Lewis, 1973), root mean squared error of approximation RMSEA (Steiger, 1990), and standardized root mean square residual (SRMR, Bentler, 1995).

The data was then simulated to represent invariance across all model steps, small, medium, and large invariance using \(d_{MACS}\) estimated sizes from Nye et al. (2019). While \(d_{MACS}\) is used primarily for an effect size of the (non)-invariance for intercepts and loadings together, a similar approach was taken for the estimation of small, medium, and large effects on the residuals. The effect size is presented for all models, calculated from the \emph{dmacs} package (Dueber, 2023; Nye \& Drasgow, 2011). Only one item in each model was manipulated from the invariant model to create the non-invariant models. Given the data was simulated with a \emph{z}-score scaling, the loading values were simulated at .30 points apart (given \(d_{MACS}\) suggestions of .2, .4, .7), the intercepts at .25 points apart, and the residuals at .25 points apart. To plan a simulation for your own study, these values can be used to simulate small, medium, and large non-invariance effects by first converting data into \emph{z}-score.

\hypertarget{package-code-examples}{%
\section{Package Code Examples}\label{package-code-examples}}

The complete code for this manuscript can be found at \url{https://osf.io/wev5f/}. This tutorial was registered at \url{https://osf.io/vwf4d}, and the example provided at the end of the manuscript was added after that registration. The \emph{R} package and replication/effect sizes was added after the original manuscript submission.

\hypertarget{multi-group-cfa-caculation}{%
\subsection{Multi-group CFA Caculation}\label{multi-group-cfa-caculation}}

First, we would create our model code in \emph{lavaan} syntax (Rosseel, 2012). The \texttt{lv} latent variable predicts the five measured variables, which are present as columns in our \texttt{df.invariant} data set. The package does generally require raw data for bootstrapping purposes, and an example of how to simulate data from models and covariance/correlations tables sometimes provided in manuscripts (rather than the raw data) is provided in the supplemental documentation.

\emph{lavaan} automatically sets the mean (i.e., the intercept) for latent variables to zero. If we wish to visualize the impact of the changes in parameter estimates across groups on the latent means, we need to allow the latent mean estimation with \texttt{lv\ \textasciitilde{}\ 1}. However, adding this estimation into our model will create a non-identified model. To solve this problem, you can set one of the intercepts of another variable to a value to scale the model. Here we will set the scale of the model by using \texttt{q1\ \textasciitilde{}\ 0*1}, thus, scaling the expected means to zero. With simulation, this step is easy to know which variable to pick - we set the intercept on the variable we know did not show differences. In real data, you may wish to run the model steps \emph{without} setting this option, examine the results of a configural or separate models, and then add the option for the values most similar. Additionally, you could complete partial invariance steps to determine which value appears most consistent to fix the estimate.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create lavaan model}
\NormalTok{model.overall }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{\# overall one{-}factor model}
\StringTok{lv =\textasciitilde{} q1 + q2 + q3 + q4 + q5}
\StringTok{\# set the intercept (mean) of q1 to zero}
\StringTok{q1 \textasciitilde{} 0*1}
\StringTok{\# allow the lv intercept to be freely estimated}
\StringTok{lv \textasciitilde{} 1"}
\CommentTok{\# look at the data}
\FunctionTok{head}\NormalTok{(df.invariant)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
##           q1          q2          q3         q4         q5   group
## 1 -0.8903542 -0.81707530  0.06137292 -1.3236407 -1.7916418 Group 1
## 2  1.1054521 -0.03540948 -0.81299606  1.0028340 -0.1909127 Group 1
## 3  1.4555852  1.54083484  1.59084213 -0.3345967 -0.6865496 Group 1
## 4 -1.8745187 -1.27880245 -2.53565792 -1.0024193 -1.6253249 Group 1
## 5 -0.4449517 -0.17782974  1.05507079 -1.2615705  1.7536428 Group 1
## 6  0.2278813  0.71348845  1.63251893  0.6449847 -1.0055700 Group 1
\end{verbatim}

The \texttt{mgcfa} function is designed to flexibly allow you to leverage \texttt{lavaan}'s package functions to calculate multiple measurement steps at once. You would include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  the model syntax in the \texttt{model} argument
\item
  the dataframe in the \texttt{data} argument of our function
\item
  the name of the grouping variable in quotes for \texttt{group}
\item
  and the equality constraints you would like to impose in order in \texttt{group.equal}
\item
  \texttt{...} any other \emph{lavaan} arguments you would like to use such as \texttt{meanstructure} or \texttt{estimator}.
\end{enumerate}

Note: you can also use \texttt{sample.cov}, \texttt{sample.mean}, \texttt{sample.nobs} in this step for estimation of multi-group models, but simulated dataframes are needed for bootstrapping replication estimates.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# run our mgcfa function to run all models}
\NormalTok{results.invariant }\OtherTok{\textless{}{-}} 
  \CommentTok{\# name of the saved model syntax}
  \FunctionTok{mgcfa}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model.overall, }
        \CommentTok{\# name of the dataframe}
        \AttributeTok{data =}\NormalTok{ df.invariant,}
        \CommentTok{\# name of the grouping variable}
        \AttributeTok{group =} \StringTok{"group"}\NormalTok{,}
        \CommentTok{\# equality constraints to impose in order}
        \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{),}
        \CommentTok{\# other options to send to lavaan cfa function}
        \AttributeTok{meanstructure =}\NormalTok{ T)}

\CommentTok{\# what is saved for you}
\FunctionTok{names}\NormalTok{(results.invariant)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## [1] "model_coef"        "model_fit"         "model_overall"    
## [4] "group_models"      "model_configural"  "invariance_models"
\end{verbatim}

The following output is saved:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{model\_coef}: The parameter estimates for each model with the model step included in a \emph{model} column. This set of coefficients can be used for other functions. This dataframe is created with \emph{broom}'s \texttt{tidy()} function if you wish to recreate this table without running the \texttt{mgcfa()} function (Robinson et al., 2023).
\end{enumerate}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results.invariant}\SpecialCharTok{$}\NormalTok{model\_coef[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{ , ]}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 10 x 13
##    term     op    estimate std.error statistic   p.value  std.lv std.all std.nox
##    <chr>    <chr>    <dbl>     <dbl>     <dbl>     <dbl>   <dbl>   <dbl>   <dbl>
##  1 "lv =~ ~ =~      1         0         NA     NA         0.803   0.616   0.616 
##  2 "lv =~ ~ =~      0.655     0.0880     7.44   9.77e-14  0.526   0.493   0.493 
##  3 "lv =~ ~ =~      0.640     0.0895     7.15   8.83e-13  0.514   0.463   0.463 
##  4 "lv =~ ~ =~      0.277     0.0749     3.69   2.24e- 4  0.222   0.209   0.209 
##  5 "lv =~ ~ =~      0.955     0.117      8.13   4.44e-16  0.766   0.656   0.656 
##  6 "q1 ~1 " ~1      0         0         NA     NA         0       0       0     
##  7 "lv ~1 " ~1     -0.0305    0.0582    -0.524  6.00e- 1 -0.0380 -0.0380 -0.0380
##  8 "q1 ~~ ~ ~~      1.05      0.0995    10.6    0         1.05    0.620   0.620 
##  9 "q2 ~~ ~ ~~      0.860     0.0653    13.2    0         0.860   0.757   0.757 
## 10 "q3 ~~ ~ ~~      0.966     0.0711    13.6    0         0.966   0.785   0.785 
## # i 4 more variables: model <chr>, block <int>, group <int>, label <chr>
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{model\_fit}: The model fit indices from \texttt{fitmeasures()} to review for overall model fit and invariance judgments. The name of the model is included in a \emph{model} column.
\end{enumerate}

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(results.invariant}\SpecialCharTok{$}\NormalTok{model\_fit)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 6 x 18
##    agfi   AIC   BIC   cfi  chisq  npar  rmsea rmsea.conf.high    srmr   tli
##   <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>           <dbl>   <dbl> <dbl>
## 1 0.998 7516. 7580. 1      0.650    15 0               0      0.00616 1.04 
## 2 0.948 3766. 3819. 0.976  7.79     15 0.0473          0.108  0.0312  0.953
## 3 0.974 3768. 3820. 1      4.48     15 0               0.0831 0.0210  1.01 
## 4 0.961 7533. 7660. 0.991 12.3      30 0.0301          0.0785 0.0261  0.982
## 5 0.965 7528. 7638. 0.994 15.4      26 0.0200          0.0660 0.0330  0.992
## 6 0.969 7522. 7615. 1     17.3      22 0               0.0542 0.0352  1.00 
## # i 8 more variables: converged <lgl>, estimator <chr>, ngroups <int>,
## #   missing_method <chr>, nobs <int>, norig <int>, nexcluded <int>, model <chr>
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \texttt{model\_overall}: A saved \emph{lavaan} fitted model of all groups together without any equality constraints or grouping variables. These objects can be used with any function that normally takes a saved model: \texttt{parameterEstimates()}, \texttt{modificationIndices()}, \texttt{semPlot::semPaths()}, and so on (Epskamp, 2022).
\item
  \texttt{group\_models}: A list of saved fitted models for each group separately.
\item
  \texttt{model\_configural}: A saved fitted model for the configural model that nests together each group into one model with no other constraints.
\item
  \texttt{invariance\_models}: A list of saved fitted models that consecutively adds \texttt{group.equal} constraints.
\end{enumerate}

\hypertarget{visualization-of-invariance}{%
\subsection{Visualization of Invariance}\label{visualization-of-invariance}}

\hypertarget{package-function}{%
\subsubsection{Package Function}\label{package-function}}

The results from the \texttt{model\_coef} table can then be used directly in \texttt{plot\_mi()}. The plot outputs will be described below. First, here are the arguments for the function:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{data\_coef}: A tidy dataframe of the parameter estimates from the models. This function assumes you have used \texttt{broom::tidy()} on the saved model from \emph{lavaan} and added a column called ``model'' with the name of the model step (Robinson et al., 2023). This function will only run for models that have used the grouping function (i.e., configural, metric, scalar, and strict or other combinations/steps you wish to examine).
\item
  \texttt{model\_step}: Which model do you want to plot? You should match this name to the one you want to extract from your model column in the \texttt{data\_coef}.
\item
  \texttt{item\_name}: Which observed variable from your model syntax do you want to plot? Please list this variable name exactly how it appears in the model.
\item
  \texttt{x\_limits}: What do you want the x-axis limits to be for your invariance plot? The default option is to assume the latent variable is standardized, and therefore, -1 to 1 is recommended. Use only two numbers, a lower and upper limit. This value also constrains the latent mean diagram to help zoom in on group differences because the scale of latent means is usually centered over zero. You can use this parameter to zoom out to a more traditional histogram using \texttt{c(-2,\ 2)}.
\item
  \texttt{y\_limits}: What do you want the y-axis limits to be for your invariance plot? Given that the latent variable is used to predict the observed values in the data, you could use the minimum and maximum values found in the data. If that range is large, consider reducing this value to be able to visualize the results (i.e., otherwise it may be too zoomed out to judge group differences). Use only two numbers, a lower and upper limit.
\item
  \texttt{conf.level}: What confidence limit do you want to plot? Use 1 - \(\alpha\).
\item
  \texttt{model\_results}: In this argument, include the saved \emph{lavaan} output for the model listed in the \texttt{model\_step} argument.
\item
  \texttt{lv\_name}: Include the name of the latent variable, exactly how it is listed in your \emph{lavaan} syntax. You should plot the latent variable that the \texttt{item\_name} is linked to. If you have items that load onto multiple latent variables, you will need to make multiple plots.
\item
  \texttt{plot\_groups}: If you include more than two groups in a multi-group model, the automatic assumption is that you want the first two groups for this visualization. If not, include the names of the groups here to plot.
\end{enumerate}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{invariant.plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{plot\_mi}\NormalTok{(}
    \CommentTok{\# output from model\_coef}
    \AttributeTok{data\_coef =}\NormalTok{ results.invariant}\SpecialCharTok{$}\NormalTok{model\_coef, }
    \CommentTok{\# which model do you want to plot}
    \AttributeTok{model\_step =} \StringTok{"Configural"}\NormalTok{, }
    \CommentTok{\# name of observed item}
    \AttributeTok{item\_name =} \StringTok{"q4"}\NormalTok{, }
    \CommentTok{\# latent variable limits to graph}
    \AttributeTok{x\_limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }
    \CommentTok{\# Y min and max in data }
    \AttributeTok{y\_limits =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(df.invariant}\SpecialCharTok{$}\NormalTok{q4), }\FunctionTok{max}\NormalTok{(df.invariant}\SpecialCharTok{$}\NormalTok{q4)),}
    \CommentTok{\# what ci do you want}
    \AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{, }
    \CommentTok{\# what model results do you want }
    \AttributeTok{model\_results =}\NormalTok{ results.invariant}\SpecialCharTok{$}\NormalTok{model\_configural,}
    \CommentTok{\# which latent variable do you want }
    \AttributeTok{lv\_name =} \StringTok{"lv"} 
\NormalTok{)}

\FunctionTok{names}\NormalTok{(invariant.plot)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## [1] "complete"  "intercept" "mean"      "variance"
\end{verbatim}

The outputs from this function are several \emph{ggplot2} objects that can be edited or saved directly using \emph{ggplot2} functionality (Wickham, 2016).

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{complete}: The output from this model can be found in Figure \ref{fig:invariant-pic}. On the left hand side, the item invariance is plotted, and on the right hand side, the latent mean distributions for the two groups are plotted. In the item invariance sub-plot, the visualization includes all three components traditionally seen in MGCFA testing steps: loadings, intercepts, and residuals. Each visualization element was designed to match the traditional visualization for that type of output. All parameter estimates are plotted on the unstandardized estimates and their confidence interval based on the standard error of the estimate. All plots are made with \emph{ggplot2} and \emph{cowplot} (Wilke, 2020).
\end{enumerate}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/invariant-pic-1.pdf}
\caption{\label{fig:invariant-pic}Invariant Model Visualization}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{intercept}: Only the left hand side of the complete plot designed to represent intercepts and factor loadings. Factor loadings represent the slope of the regression equation for the latent variable predicting the scores on the observed variable (\(\hat{Y} \sim b_0 + b_1X + \epsilon\)). The y-axis indicates the observed variable scores, and here, the plot includes the entire range of the scale of the data for item four. The coefficient (\(b_1\)) for group 1 was 0.40, while the coefficient for group 2 was 0.21. The ribbon bands around the plotted slopes indicate the confidence interval for that estimate. In this plot, while the coefficients for each group are not literally equal, the overlapping and parallel slope bands indicate they are not different practically.
\end{enumerate}

The item intercepts (\(b_0\)) are plotted on the middle line where they would cross the y-axis at a latent variable score of zero. These are represented by a dot with a set of confidence error bars around the point. The intercept for group 1 was 0.07, while the coefficient for group 2 was 0.03. In this invariant depiction, the overlap in the intercepts is clear, indicating they are not different. You can use \texttt{y\_limits} to zoom in on the graph if these are too small to be distinguishable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\item
  \texttt{mean}: The right hand side of the complete plot graphing the latent variable means and density from the data. The latent variable is shown on the x-axis using standardized values (i.e., \emph{z}-scores) where -1 indicates one standard deviation below the mean for the latent variable, 0 indicates the mean for the latent variable and so on. The lines indicate the means of the latent variables from the simulated dataset. Group labels are represented in the figure caption on the bottom. Group 1 is usually the group that is alphabetically first in the data set or whichever group is the first that appears when using the \texttt{levels()} command.
\item
  \texttt{variance}: A split geom violin plot indicating the variance distribution of the plotted item. Residuals are trickier to plot, as they are the left over error when predicting the observed variables \(\epsilon\). It is tempting to plot this value as the confidence band around the slope, however, that defeats the purpose of understanding that the slopes are estimated separately from the residuals, and both have an associated variability around their parameter estimate. Therefore, residuals are represented in the inset picture at the bottom right of the item invariance plot. The black bars represent the estimated residual for each group (group 1: 0.91, group 2: 1.22). The distributions are plotted to represent the normal spread of values using the standard error of the residuals. The violin plot allows for direct comparison of those residuals and their potential distributions. Note that the placement has nothing to do with the x or y-axis and is designed to always show in the same location, regardless of size/value. The plots are included separately so they can be arranged in a different fashion if desired.
\end{enumerate}

\hypertarget{simulated-results}{%
\subsubsection{Simulated Results}\label{simulated-results}}

The \(d_{MACS}\) value for item 4 in the invariant model was 0.16, representing a nil or unimportant difference in this manuscript. It is important to note that while Nye et al. (2019) suggests specific sizes for small, medium, and large, each researcher should determine for themselves what effects represent. Figure \ref{fig:small-load-pic} displays the results from the small (\(d_{MACS}\) = 0.27) difference in loadings, while Figure \ref{fig:med-load-pic} displays the results from the medium (\(d_{MACS}\) = 0.53) difference in loadings, and Figure \ref{fig:large-load-pic} shows the large (\(d_{MACS}\) = 0.68) differences. When investigating the slope values, we can clearly see the change in the loading for the second group (the only manipulated variable, although random data set generation may also change intercepts and residuals slightly). At the medium effect size, we see that the confidence bands do not overlap (at the edges), and at the large effect size, we can see a clear separation of two lines. Note that the intercepts in this model are estimated as equal so the loading representation will not literally separate, but the steepness of the lines is the indicator of the difference between the slopes. You can imagine these lines are interpreted like a simple slopes analysis for interactions in regression (Cohen et al., 2003). When simple slopes for interactions are plotted, if they are parallel, there is no interaction, and if they cross, then there is an interaction. Here, we can use this same logic. If they are parallel, there is likely invariance (they are the same), and the further from parallel they become, the larger the effect size for the differences between group loadings.

The latent means in Figure \ref{fig:large-load-pic} do appear to show differences, albeit visually small. The latent means diagram shows the impact of any group differences that aren't constrained, and this image shows the configural model (as the metric model would force them to be equal). In the simulated model, the \emph{only} manipulated parameter is question 4's loading. In real models, the differences may be larger due to other variation found in the parameter estimates. Therefore, once you discover items you believe would make a model ``partially'' invariant, you may wish to estimate that model and graph the item again using the partially invariant model to see only the effect of the non-invariant items. Additionally, consider that we set the scaling of the model to 0. The estimate for the lv mean in the large loading model was group 1: 0.00, and group 2: -0.06, which results in 0.06 difference in group means. The practical implications of this difference will depend on the research and interpretations of the researcher.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/small-load-pic-1.pdf}
\caption{\label{fig:small-load-pic}Small Loadings Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/med-load-pic-1.pdf}
\caption{\label{fig:med-load-pic}Medium Loadings Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/large-load-pic-1.pdf}
\caption{\label{fig:large-load-pic}Large Loadings Model Visualization}
\end{figure}

For intercepts, the small (Figure \ref{fig:small-int-pic}), medium (Figure \ref{fig:med-int-pic}), and large (Figure \ref{fig:large-int-pic}) depictions represent \(d_{MACS}\) values of 0.26, 0.47, and 0.70, respectively. Intercept differences can be clearly seen represented by the spacing out of the intercept locations (and thus, the overall line as well). While the changes in intercept do not appear to change the latent means, the caveat to this simulation is that only item four was manipulated. An example is provided below that demonstrates large changes in latent means.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/small-int-pic-1.pdf}
\caption{\label{fig:small-int-pic}Small Intercepts Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/med-int-pic-1.pdf}
\caption{\label{fig:med-int-pic}Medium Intercepts Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/large-int-pic-1.pdf}
\caption{\label{fig:large-int-pic}Large Intercepts Model Visualization}
\end{figure}

Last, the effect of the residuals is plotted in small (Figure \ref{fig:small-res-pic}), medium (Figure \ref{fig:med-res-pic}), and large (Figure \ref{fig:large-res-pic}) formats. While \(d_{MACS}\) values are not technically available for the residuals, our models showed 0.19, 0.19, and 0.16, respectively. These differences in values are variable due to the random generation of data sets for each measurement invariance manipulation. At first glance, the differences in the small chart may seem large, because the black lines are not touching, but notice that the distributions overlap, indicating a likely small difference. The medium and large differences better illustrate differences in residuals across groups. Further, the impact of the residuals on the shape of the latent mean distribution can also been seen (and unintentionally, in the first figures as well due to random variation). The impact is due to the standard error of the residuals, as smaller standard errors represent lepokurtic distributions (taller), and larger standard errors represent platykurtic distributions (flatter). The effect size difference of the residuals does not appear to change the effects in the latent means.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/small-res-pic-1.pdf}
\caption{\label{fig:small-res-pic}Small Residuals Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/med-res-pic-1.pdf}
\caption{\label{fig:med-res-pic}Medium Residuals Model Visualization}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/large-res-pic-1.pdf}
\caption{\label{fig:large-res-pic}Large Residuals Model Visualization}
\end{figure}

\hypertarget{partial-invariance-calculation}{%
\subsection{Partial Invariance Calculation}\label{partial-invariance-calculation}}

\hypertarget{package-function-1}{%
\subsubsection{Package Function}\label{package-function-1}}

The results of the simulated models are presented in the appendix, demonstrating that each simulated dataset shows partial invariance if item four is allowed to vary between groups. The function takes the following arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{saved\_model}: The saved \emph{lavaan} model with the equality constraints at the level of measurement invariance you would like to examine for partial invariance.
\item
  \texttt{data}: The dataframe where the model was estimated.
\item
  \texttt{model}: The model syntax for the overall model.
\item
  \texttt{group}: The grouping variable column in the dataframe.
\item
  \texttt{group.equal}: The equality constraints including in your original multi-group tests.
\item
  \texttt{partial\_step}: The level of partial invariance you wish to test.
\end{enumerate}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{partial.invariant }\OtherTok{\textless{}{-}}
  \FunctionTok{partial\_mi}\NormalTok{(}
    \CommentTok{\# saved model output with constraints}
    \AttributeTok{saved\_model =}\NormalTok{ results.invariant}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.residuals,}
    \CommentTok{\# dataframe from model }
    \AttributeTok{data =}\NormalTok{ df.invariant,}
    \CommentTok{\# model syntax }
    \AttributeTok{model =}\NormalTok{ model.overall,}
    \CommentTok{\# group column name }
    \AttributeTok{group =} \StringTok{"group"}\NormalTok{,}
    \CommentTok{\# group equality constraints from your mgcfa}
    \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{),}
    \CommentTok{\# which step you want to examine for partial invariance}
    \AttributeTok{partial\_step =} \StringTok{"residuals"}
\NormalTok{    )}

\FunctionTok{names}\NormalTok{(partial.invariant)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## [1] "models"    "fit_table"
\end{verbatim}

In this function, each parameter with the appropriate \emph{lavaan} syntax is relaxed individually (i.e., \texttt{\textasciitilde{}1} for intercepts, \texttt{\textasciitilde{}\textasciitilde{}} for residuals, etc.). The fitted models are saved in the \texttt{models} output, and the \texttt{fit\_table} output includes all fit indices for each model to investigate potential areas of partial invariance based on the researcher's desired criterion. Note: the \texttt{partial\_step} function is used to determine which types of \texttt{op} or operators to freely estimate between groups. If one chooses residuals, you will also freely estimate the residual for the latent variable or any other residuals found in the model. These items may be ignored if they were not meant to be included.

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(partial.invariant}\SpecialCharTok{$}\NormalTok{fit\_table }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{       dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(free.parameter, cfi, rmsea))}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 6 x 3
##   free.parameter cfi        rmsea     
##   <chr>          <lvn.vctr> <lvn.vctr>
## 1 q1 ~~ q1       0.9902679  0.02108648
## 2 q2 ~~ q2       0.9868905  0.02447336
## 3 q3 ~~ q3       0.9958241  0.01381266
## 4 q4 ~~ q4       1.0000000  0.00000000
## 5 q5 ~~ q5       0.9868088  0.02454944
## 6 lv ~~ lv       0.9906154  0.02025143
\end{verbatim}

\hypertarget{replication-and-effect-size-model}{%
\subsection{Replication and Effect Size: Model}\label{replication-and-effect-size-model}}

\hypertarget{package-function-2}{%
\subsubsection{Package Function}\label{package-function-2}}

The \texttt{bootstrap\_model} function in \emph{visualizemi} was designed to estimate the likely replication of overall model invariance with the assumption that the data used for the estimation represents the larger population. The following arguments are used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \texttt{saved\_configural}: a saved fitted model at the configural level with no equality constraints. This model should include all other lavaan settings you would like to use, such as estimator or ordered.
\item
  \texttt{data}: The dataframe where the model was estimated.
\item
  \texttt{model}: The model syntax for the overall model.
\item
  \texttt{group}: The grouping variable column in the dataframe.
\item
  \texttt{nboot}: The number of bootstraps to run.
\item
  \texttt{invariance\_index}: The fit index you would like to use to determine invariance. Please use options and labeling from \emph{lavaan} - see \texttt{fitmeasures()} for options.
\item
  \texttt{invariance\_rule}: The invariance difference score you would like to use as your rule.
\item
  \texttt{group.equal}: The equality constraints including in your original multi-group tests.
\end{enumerate}

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.model.invariant }\OtherTok{\textless{}{-}} 
  \FunctionTok{bootstrap\_model}\NormalTok{(}
    \CommentTok{\# saved configural model }
    \AttributeTok{saved\_configural =}\NormalTok{ results.invariant}\SpecialCharTok{$}\NormalTok{model\_configural,}
    \CommentTok{\# dataframe}
    \AttributeTok{data =}\NormalTok{ df.invariant,}
    \CommentTok{\# model syntax}
    \AttributeTok{model =}\NormalTok{ model.overall, }
    \CommentTok{\# group variable column in dataframe}
    \AttributeTok{group =} \StringTok{"group"}\NormalTok{,}
    \CommentTok{\# number of bootstraps}
    \AttributeTok{nboot =} \DecValTok{1000}\NormalTok{, }
    \CommentTok{\# which fit index you would like to use}
    \AttributeTok{invariance\_index =} \StringTok{"cfi"}\NormalTok{,}
    \CommentTok{\# what is your criterion for that fit index}
    \AttributeTok{invariance\_rule =}\NormalTok{ .}\DecValTok{01}\NormalTok{,}
    \CommentTok{\# what equality constraints are you testing }
    \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\normalsize

The data included in this function will be sampled, with replacement, at the same size as the current dataset, and the included invariance equality constraints are estimated. Each step will be compared to the previous step using the invariance index and comparison rule entered. The output is a dataframe of the proportion of non-invariant bootstraps from the real data and the same bootstrapped dataset with the group labels randomly assigned. The effect size comparison of proportions, \(h\), for non-invariant comparisons:

\[h_{nmi} = 2 \times (asin\sqrt{p_{data}} - asin\sqrt{p_{random}})\]

The alternative, \(h_{mi}\), for effect size of measurement invariance replication would simply be the inverse sign of \(h_{nmi}\) and is also included in the table. Two additional columns \(h_{nmi_p}\) and \(h_{nmi_p}\) represent the \(h\) values divided by the upper bound of \(h\) (i.e., \(\pi\)), to help with interpretation of the effect size (thus, bounding \(h\) to -1 to 1).

\hypertarget{simulated-results-1}{%
\subsubsection{Simulated Results}\label{simulated-results-1}}

Figure \ref{fig:boot-rr-pic} portrays the \(h_{nmi_p}\) values by simulated non-invariance, strength of non-invariance, and type of equality constraint. This image represents 100 simulations of data by 1000 bootstrapped runs (averaged) to explore the expected pattern of results. The bars are arranged to show what a researcher might inspect when thinking about replication possibilities and their effect sizes (i.e., only three bars for each equality constraint would be calculated).

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/boot-rr-pic-1.pdf}
\caption{\label{fig:boot-rr-pic}Visualization of the effect size of bootstrapped replication proportions on simulated data. Each panel indicates the simulated data type, colors represent the differences in the strength of the non-invariance, and the bars on the x-axis represent the effect size for the equality constraint.}
\end{figure}

In the data that was simulated to be invariant between groups, effect sizes are still non-zero (loadings \(h_{nmi_p}\) = 0.28, intercepts = \(h_{nmi_p}\) = 0.06, \(h_{nmi_p}\) = 0.00). This result mirrors the effects found in the literature - that often, many models fail to show invariance, and potentially not because measurement is poor but because of natural random variation in parameter estimates. This result also indicates the need to be able to identify if specific parameters are driving the differences, which is shown in the next section.

Next, Figure \ref{fig:boot-rr-pic} demonstrates the patterns one might find for small, medium, and large effects at each type of invariance when data is simulated with \emph{one} difference. For loadings, the pattern shows a larger effect for loadings with zero or negative effect sizes for other effect sizes. The intercept simulations show non-zero effect sizes in the loadings and intercepts, likely for the same reasons \(d_{MACS}\) is interpreted as a combined effect size. When intercepts are changed, loadings may naturally shift with those means. Last, the residual results present an unexpected pattern, wherein the effect is primarily seen in the loadings, rather than the residuals step. However, when distributions of error variance are different, one may expect that those effects are pushed toward the loadings as well (as values can vary more, thus potentially weakening the relationship between observed and latent variable).

An example of interpretation on real data is given in a later section. From a research study, only one effect size for each equality constraint would be calculated. The interpretation will often be up to the researcher's smallest effect of interest, and this simulation gives some guidance that the values should not be interpreted with traditional rules of thumb. The pattern of effects is potentially the most useful information: 1) positive effects on the loadings with negative or very close to zero effects on the other parameters may indicate a non-replication in loadings, 2) equal effects on loadings and intercepts with smaller or negative effects may indicate intercepts may be an issue, and 3) residuals may be determined by the same pattern as loadings but with a smaller ratio of loadings to residuals effect (i.e., loadings \(h_{nmi}\) / residuals \(h_{nmi}\). The ``size'' could be determined by the ratio of effect sizes for each constraint. Of course, this represents one simulation study, and results from many studies in a meta-analysis would be fruitful for future work.

\hypertarget{replication-and-effect-size-parameters}{%
\subsection{Replication and Effect Size: Parameters}\label{replication-and-effect-size-parameters}}

\hypertarget{package-function-3}{%
\subsubsection{Package Function}\label{package-function-3}}

After examining the overall model potential replication effect size, the individual parameters within a model can be bootstrapped for partial invariance to with that parameter relaxed (overall partial model statistics) and the difference in group parameter estimates (parameter effect size). This function uses arguments seen in other functions, so they will not be repeated here. The general setup consists of using the model you think could be partially invariant in the \texttt{saved\_model} argument and the fit index for comparison for the model with less constraints in \texttt{invariance\_compare}. This example examines the loadings in the invariant model, so \texttt{saved\_model} uses the \texttt{mgcfa} output for equality constraints present on the loadings and compares that model to the configural model with no equality constraints on the loadings. The \texttt{partial\_step} argument will be used to determine which operation syntax (i.e.~\texttt{=\textasciitilde{}} for loadings) to relax for modeling.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.partial.invariant }\OtherTok{\textless{}{-}} 
      \FunctionTok{bootstrap\_partial}\NormalTok{(}
        \CommentTok{\# saved model you want to examine the partial loadings for }
        \AttributeTok{saved\_model =}\NormalTok{ results.invariant}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.loadings,}
        \CommentTok{\# the dataset }
        \AttributeTok{data =}\NormalTok{ df.invariant, }
        \CommentTok{\# the model }
        \AttributeTok{model =}\NormalTok{ model.overall,}
        \CommentTok{\# the group variable in the dataset}
        \AttributeTok{group =} \StringTok{"group"}\NormalTok{, }
        \CommentTok{\# number of bootstraps}
        \AttributeTok{nboot =} \DecValTok{1000}\NormalTok{,}
        \CommentTok{\# which fit index you would like to use to determine partial invariance}
        \AttributeTok{invariance\_index =} \StringTok{"cfi"}\NormalTok{, }
        \CommentTok{\# what is the invariance rule }
        \AttributeTok{invariance\_rule =}\NormalTok{ .}\DecValTok{01}\NormalTok{, }
        \CommentTok{\# what are we comparing the saved model fit index to }
        \AttributeTok{invariance\_compare =} \FunctionTok{fitmeasures}\NormalTok{(results.invariant}\SpecialCharTok{$}\NormalTok{model\_configural, }\StringTok{"cfi"}\NormalTok{), }
        \CommentTok{\# what step are we using for invariance}
        \AttributeTok{partial\_step =} \StringTok{"loadings"}\NormalTok{, }
        \CommentTok{\# what equality constraints should be imposed }
        \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{)}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

\normalsize

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(boot.partial.invariant)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## [1] "invariance_plot"        "effect_invariance_plot" "density_plot"          
## [4] "boot_DF"                "boot_summary"           "boot_effects"
\end{verbatim}

The saved output includes several dataframes and plots. The first is the \texttt{boot\_DF} which the summary of each run in a dataframe for plotting or summarization. This dataframe includes the estimate for each paramter (\texttt{term}) separated by group and type (\texttt{boot\_1}, \texttt{boot\_2} are the bootstrapped estimates for group 1 and group 2, while the same \texttt{random} columns indicate the randomly assigned groups). The fit index used to determine invariance is included for bootstrapped and random estimates, and then the differences between groups and if they were ``invariant'' or not given the researcher supplied rule.

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(boot.partial.invariant}\SpecialCharTok{$}\NormalTok{boot\_DF)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
##       term    boot_1     boot_2  random_1  random_2  boot_fit random_fit
## 1 lv =~ q1 0.4548783 0.49928877 0.4627486 0.4651391 0.9296990  1.0000000
## 2 lv =~ q2 0.3599017 0.56241016 0.4100874 0.4980215 0.9441125  1.0000000
## 3 lv =~ q3 0.4254283 0.33640233 0.4274329 0.3422124 0.9377130  1.0000000
## 4 lv =~ q4 0.3930716 0.03320619 0.1380833 0.2628802 0.9750274  1.0000000
## 5 lv =~ q5 0.7306414 0.73512673 0.7093891 0.7532471 0.9266587  1.0000000
## 6 lv =~ q1 0.5537083 0.57086815 0.5732166 0.5475714 0.8958929  0.9814658
##   boot_difference random_difference boot_index_difference
## 1    -0.044410454      -0.002390463                 FALSE
## 2    -0.202508484      -0.087934027                 FALSE
## 3     0.089025927       0.085220565                 FALSE
## 4     0.359865463      -0.124796846                 FALSE
## 5    -0.004485377      -0.043857947                 FALSE
## 6    -0.017159815       0.025645271                 FALSE
##   random_index_difference
## 1                    TRUE
## 2                    TRUE
## 3                    TRUE
## 4                    TRUE
## 5                    TRUE
## 6                    TRUE
\end{verbatim}

Next, the \texttt{boot\_summary} includes a summarized form of the bootstrapped results from separated by bootstrapping versus random and invariant/non-invariant. The \(d_s\) for between groups Cohen's \(d\) is shown below, and the non-central confidence interval is included. Effect sizes are only calculated when the number of bootstrapped estimates is at least 10\% of the data - therefore, you would not receive effect sizes with almost no bootstrapped runs. This dataframe should be used to determine which parameter may be different and at what size between groups in a replication of the study.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.partial.invariant}\SpecialCharTok{$}\NormalTok{boot\_summary }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(term, d\_boot, d\_random)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 10 x 4
## # Groups:   term, invariant [10]
##    invariant term       d_boot d_random
##    <lgl>     <chr>       <dbl>    <dbl>
##  1 FALSE     lv =~ q1 -0.0299   0.0583 
##  2 TRUE      lv =~ q1  0.0337   0.0116 
##  3 FALSE     lv =~ q2 -0.0326   0.0933 
##  4 TRUE      lv =~ q2  0.146    0.0309 
##  5 FALSE     lv =~ q3 -0.0463   0.113  
##  6 TRUE      lv =~ q3 -0.148    0.0743 
##  7 FALSE     lv =~ q4  0.00785 -0.0668 
##  8 TRUE      lv =~ q4 -0.0157   0.0389 
##  9 FALSE     lv =~ q5 -0.00129 -0.169  
## 10 TRUE      lv =~ q5  0.122   -0.00853
\end{verbatim}

The \texttt{boot\_effects} table creates a summary similar to the overall model replication table based on the proportion of runs that were considered invariant versus not for each parameter. Note that the effects match the overall results, such that simulated invariant data appears to still show the likelihood that loadings may not replicate in a similar dataset.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.partial.invariant}\SpecialCharTok{$}\NormalTok{boot\_effects}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 5 x 7
##   term     non_invariant random_non_invariant h_nmi  h_mi h_nmi_p h_mi_p
##   <chr>            <dbl>                <dbl> <dbl> <dbl>   <dbl>  <dbl>
## 1 lv =~ q1         0.853                0.236  1.34 -1.34   0.427 -0.427
## 2 lv =~ q2         0.858                0.237  1.35 -1.35   0.430 -0.430
## 3 lv =~ q3         0.851                0.23   1.35 -1.35   0.429 -0.429
## 4 lv =~ q4         0.84                 0.229  1.32 -1.32   0.420 -0.420
## 5 lv =~ q5         0.819                0.237  1.25 -1.25   0.397 -0.397
\end{verbatim}

Plots of the results from dataframes can be found within the \texttt{bootstrap\_partial()} function. Figure \ref{fig:invariance-partial-fig} shows the difference between parameters for groups in the bootstrapped and randomly assigned group runs. Figure \ref{fig:density-partial-fig} shows the density plot of the estimates for each group organized by bootstrapped and randomly assigned groups and the invariance decision for each bootstrapped run. Last, Figure \ref{fig:effect-partial-fig} indicates the \(d_s\) value between groups with an indication of the number of data points in each estimate (i.e., dot size). These visualizations should allow a researcher to understand the likelihood of replication for each parameter, as well as the potential size of the differences. Therefore, one could indicate a specific smallest effect size of interest, rather than a invariance cut-off rule of thumb when planning a replication or registered report.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/invariance-partial-fig-1.pdf}
\caption{\label{fig:invariance-partial-fig}Visualization of the difference score between groups by parameter for invariant and non-invariant bootstrapped and randomly assigned group data.}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/density-partial-fig-1.pdf}
\caption{\label{fig:density-partial-fig}Visualization of the number of estimates for each group by bootstrapped and randomly assigned group runs by their invariance decision.}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/effect-partial-fig-1.pdf}
\caption{\label{fig:effect-partial-fig}Visualization of effect size between groups by parameter for invariant and non-invariant bootstrapped and randomly assigned group data. The size of the dots indicate the number of data points for that estimate.}
\end{figure}

\hypertarget{simulated-results-2}{%
\subsubsection{Simulated Results}\label{simulated-results-2}}

Figure \ref{fig:effect-large-loading-pic} shows the effect size differences within large loadings simulations. The results demonstrate that most of the loadings were considered non-invariant in the bootstrapped models (while holdings all others equal). This result is partially due to simulating very good data, so small changes in loadings results in a drop in fit for our chosen invariance index. However, we can use this graph to show that question four shows a possible effect size ranging from -0.07 to 0.13. The \(h_{nmi_p}\) value for question four was 0.27, representing about a quarter of a possible total effect. Last, the density plot in Figure \ref{fig:density-large-loading-pic} shows the separation of the two different groups loadings in item four, thus, illustrating group differences in the findings for their loadings. Each of the other combination of plots can be found in the supplemental materials.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/effect-large-loading-pic-1.pdf}
\caption{\label{fig:effect-large-loading-pic}Bootstrapped and Random Group effect size differences in loadings for the Large Loading difference simulation. The size of the point reprensents the number of data points included in that calculation.}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/density-large-loading-pic-1.pdf}
\caption{\label{fig:density-large-loading-pic}Bootstrapped and Random density plots for invariant and non-invariant bootstrapped partial effects examining only large loadings.}
\end{figure}

\hypertarget{an-example-analysis}{%
\subsection{An Example Analysis}\label{an-example-analysis}}

Aiena et al. (2014) examined the RS-14 (Wagnild, 2009) exploring the factor structure of the Resiliency Scale in a clinical sample receiving treatment services and a college student sample. Measurement invariance was calculated for differences separately for these samples for gender and race finding a partially invariant models with a few item intercepts or residuals that differed between groups. Aiena et al. (2014) did not compare the clinical to the student sample for measurement invariance, and it is reasonable to expect potential differences in these two populations. This example will demonstrate the procedure for researchers who wish to use partial invariance steps and how to interpret real, messy data.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the data, it is called DF}
\FunctionTok{load}\NormalTok{(}\StringTok{"manu\_data/RS14.Rdata"}\NormalTok{)}

\CommentTok{\# build the one{-}factor model }
\NormalTok{model.rs }\OtherTok{\textless{}{-}} \StringTok{"RS =\textasciitilde{} RS1+RS2+RS3+RS4+RS5+RS6+RS7+RS8+RS9+RS10+RS11+RS12+RS13+RS14"}
\CommentTok{\# run the multi{-}group CFA}
\NormalTok{results.rs }\OtherTok{\textless{}{-}} \FunctionTok{mgcfa}\NormalTok{(}
  \AttributeTok{model =}\NormalTok{ model.rs,}
  \AttributeTok{data =}\NormalTok{ DF, }
  \AttributeTok{group =} \StringTok{"sample"}\NormalTok{, }
  \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\normalsize

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# how to get results in table }
\NormalTok{results.rs}\SpecialCharTok{$}\NormalTok{model\_fit }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{        dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(model, AIC, BIC, cfi, tli, rmsea, srmr)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:rs-table}Model Fit for RS-14 Example}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 126,722.491 & 126,888.707 & 0.934 & 0.923 & 0.094 & 0.036\\
Group Clinical & 52,961.421 & 53,099.720 & 0.919 & 0.904 & 0.090 & 0.044\\
Group Student & 69,100.985 & 69,254.310 & 0.928 & 0.915 & 0.108 & 0.035\\
Configural & 122,118.406 & 122,617.055 & 0.926 & 0.912 & 0.102 & 0.036\\
Loadings & 122,144.532 & 122,566.010 & 0.925 & 0.918 & 0.098 & 0.043\\
Intercepts & 122,544.109 & 122,888.415 & 0.911 & 0.910 & 0.103 & 0.052\\
Residuals & 126,466.241 & 126,727.438 & 0.780 & 0.793 & 0.156 & 0.086\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

Table \ref{tab:rs-table} indicates the results after running the one-factor model. There are several guidelines for assessing assessing a degradation in model fit (Cao \& Liang, 2022; Cheung \& Rensvold, 2002; Counsell et al., 2020; Jin, 2020; Putnick \& Bornstein, 2016) but for the purposes of this illustration \(\Delta\)CFI \textgreater{} .01 will be used. Table \ref{tab:rs-table} indicates that fit was degraded when the constraint on equal item intercepts was added. The code below provides an example of testing each item individually by relaxing the constraints and recalculating the CFI. If these Items bring the CFI value back up to \(\Delta\)CFI \textless= .01 from the metric model, then the model would be considering partially invariant at the scalar level. It seems unlikely that the residuals will show invariance, if partial scalar invariance can be found, as the drop in fit is quite large.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{partial.rs }\OtherTok{\textless{}{-}}
  \FunctionTok{partial\_mi}\NormalTok{(}
    \AttributeTok{saved\_model =}\NormalTok{ results.rs}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts,}
    \AttributeTok{data =}\NormalTok{ DF,}
    \AttributeTok{model =}\NormalTok{ model.rs,}
    \AttributeTok{group =} \StringTok{"sample"}\NormalTok{,}
    \CommentTok{\# be sure to do only up to the step you are interested in}
    \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{),}
    \AttributeTok{partial\_step =} \StringTok{"intercepts"}\NormalTok{)}

\NormalTok{partial.rs}\SpecialCharTok{$}\NormalTok{fit\_table }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(free.parameter, cfi)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 15 x 2
##    free.parameter cfi       
##    <chr>          <lvn.vctr>
##  1 "RS1 ~1 "      0.9116914 
##  2 "RS2 ~1 "      0.9129976 
##  3 "RS3 ~1 "      0.9117235 
##  4 "RS4 ~1 "      0.9111212 
##  5 "RS5 ~1 "      0.9126742 
##  6 "RS6 ~1 "      0.9133618 
##  7 "RS7 ~1 "      0.9139287 
##  8 "RS8 ~1 "      0.9111397 
##  9 "RS9 ~1 "      0.9119702 
## 10 "RS10 ~1 "     0.9118309 
## 11 "RS11 ~1 "     0.9110574 
## 12 "RS12 ~1 "     0.9112309 
## 13 "RS13 ~1 "     0.9112367 
## 14 "RS14 ~1 "     0.9112015 
## 15 "RS ~1 "       0.9108805
\end{verbatim}

The output indicates that RS6 and RS7 are potential items that could be relaxed to improve model fit and create a partial scalar invariant model (i.e., by picking the largest CFI values). The code below show to check the addition of these items, which are added one at a time. You use the \texttt{group.partial} open to ``relax'' or freely estimate that parameter for each group separately.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# run the partially invariant model with group.partial}
\NormalTok{partial.rs}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{mgcfa}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model.rs, }
                  \AttributeTok{data =}\NormalTok{ DF, }
                  \AttributeTok{group =} \StringTok{"sample"}\NormalTok{, }
                  \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{),}
                  \AttributeTok{group.partial =} \FunctionTok{c}\NormalTok{(}\StringTok{"RS7\textasciitilde{}1"}\NormalTok{),}
                  \AttributeTok{meanstructure =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# examine the loadings }
\NormalTok{partial.rs}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{model\_coef }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{==} \StringTok{"RS7 \textasciitilde{}1 "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(model }\SpecialCharTok{==} \StringTok{"intercepts"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(term, group, estimate, std.error)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 2 x 4
##   term      group estimate std.error
##   <chr>     <int>    <dbl>     <dbl>
## 1 "RS7 ~1 "     1     4.95    0.0580
## 2 "RS7 ~1 "     2     4.49    0.0529
\end{verbatim}

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# examine the fit indices }
\NormalTok{partial.rs}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{model\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(model }\SpecialCharTok{==} \StringTok{"intercepts"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(AIC, BIC, cfi, tli, rmsea, srmr)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 1 x 6
##       AIC     BIC   cfi   tli rmsea   srmr
##     <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl>
## 1 122454. 122804. 0.914 0.912 0.102 0.0502
\end{verbatim}

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size model }
\FunctionTok{lavaan\_dmacs}\NormalTok{(partial.rs}\FloatTok{.1}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts, }\StringTok{"Clinical"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{DMACS[}\DecValTok{7}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
##      RS7 
## 0.282302
\end{verbatim}

By examining our estimates, we can see that item seven on the RS-14 is estimated at nearly 5 points for the clinical sample, while the student sample has a lower mean around 4.5 points. Generally, students show higher means on the items of the RS-14, but when all loadings and other intercepts are constrained to be equal, and this one item is relaxed, this pattern flips so that clinical groups show higher item intercepts. Given the scale is a 1-7 Likert type scale, .5 a point represents a potentially sizable change on the scale. Item seven covers perseverance after hardship, and all items can be found in the user manual for the scale at www.resiliencecenter.com. The effect size from \(d_{MACS}\) suggests a small to medium effect, 0.28. In this next code section, we repeat this process for the RS6, as the CFI for our model with only RS7 does not achieve the levels of partial invariance for our \(\Delta\)CFI criterion (i.e., \textless= .01 downward change in fit: metric CFI = .925, partial scalar CFI = .914). See Figure \ref{fig:rs7-img} for the difference between item intercepts and latent means.

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add the second intercept}
\NormalTok{partial.rs}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{mgcfa}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model.rs, }
                  \AttributeTok{data =}\NormalTok{ DF, }
                  \AttributeTok{group =} \StringTok{"sample"}\NormalTok{, }
                  \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{),}
                  \AttributeTok{group.partial =} \FunctionTok{c}\NormalTok{(}\StringTok{"RS7\textasciitilde{}1"}\NormalTok{, }\StringTok{"RS6\textasciitilde{}1"}\NormalTok{),}
                  \AttributeTok{meanstructure =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# examine the loadings }
\NormalTok{partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{model\_coef }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{==} \StringTok{"RS6 \textasciitilde{}1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(model }\SpecialCharTok{==} \StringTok{"intercepts"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(term, group, estimate, std.error)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 0 x 4
## # i 4 variables: term <chr>, group <int>, estimate <dbl>, std.error <dbl>
\end{verbatim}

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# examine the fit indices }
\NormalTok{partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{model\_fit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(model }\SpecialCharTok{==} \StringTok{"intercepts"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(AIC, BIC, cfi, tli, rmsea, srmr)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 1 x 6
##       AIC     BIC   cfi   tli rmsea   srmr
##     <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl>
## 1 122363. 122719. 0.917 0.915 0.100 0.0488
\end{verbatim}

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size model }
\FunctionTok{lavaan\_dmacs}\NormalTok{(partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts, }\StringTok{"Clinical"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{DMACS[}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
##       RS6 
## 0.2796334
\end{verbatim}

Again, we see about a half-point difference between our clinical and student samples for item 6, which is about drive to achieve. The CFI for this model does meet the requirements for partial invariance, .917. The effect size is approximately the same at \(d_{MACS}\) = 0.28. See Figure \ref{fig:rs6-img} shows the difference between item intercepts and latent means.

\normalsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the image for RS7}
\FunctionTok{plot\_mi}\NormalTok{(}
  \AttributeTok{data\_coef =}\NormalTok{ partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{model\_coef, }
  \AttributeTok{model\_step =} \StringTok{"intercepts"}\NormalTok{, }
  \AttributeTok{item\_name =} \StringTok{"RS7"}\NormalTok{, }
  \AttributeTok{x\_limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
  \AttributeTok{y\_limits =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(DF}\SpecialCharTok{$}\NormalTok{RS7), }\FunctionTok{max}\NormalTok{(DF}\SpecialCharTok{$}\NormalTok{RS7)),}
  \AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{, }
  \AttributeTok{model\_results =}\NormalTok{ partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts,}
  \CommentTok{\# which latent variable do you want }
  \AttributeTok{lv\_name =} \StringTok{"RS"} 
\NormalTok{)}\SpecialCharTok{$}\NormalTok{complete}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs7-img-1.pdf}
\caption{\label{fig:rs7-img}RS7 Item Invariance Visualization}
\end{figure}

\normalsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the image for RS6}
\FunctionTok{plot\_mi}\NormalTok{(}
  \AttributeTok{data\_coef =}\NormalTok{ partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{model\_coef, }
  \AttributeTok{model\_step =} \StringTok{"intercepts"}\NormalTok{, }
  \AttributeTok{item\_name =} \StringTok{"RS6"}\NormalTok{, }
  \AttributeTok{x\_limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
  \AttributeTok{y\_limits =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(DF}\SpecialCharTok{$}\NormalTok{RS6), }\FunctionTok{max}\NormalTok{(DF}\SpecialCharTok{$}\NormalTok{RS6)),}
  \AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{, }
  \AttributeTok{model\_results =}\NormalTok{ partial.rs}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts,}
  \CommentTok{\# which latent variable do you want }
  \AttributeTok{lv\_name =} \StringTok{"RS"} 
\NormalTok{)}\SpecialCharTok{$}\NormalTok{complete}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs6-img-1.pdf}
\caption{\label{fig:rs6-img}RS6 Item Invariance Visualization}
\end{figure}

Next, we would examine our replication potential for this model. Given our current results, we may not expect our intercepts to replicate. Given the order of \texttt{group.equal}, the boot function will select the first non-invariant step in the calculation of the effect size for potential replication. In our output, we do not see a loadings effect size, and this result occurs when \emph{none} of the bootstrapped or random results are non-invariant. Therefore, we would expect the loadings to replicate (and the effect size would be 0 difference between bootstrapped and random, both showing invariance). The intercepts show a large (i.e., close to the max possible value) non-invariant effect, and therefore, we should not expect this model to show invariance in a replication.

\normalsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.model.rs }\OtherTok{\textless{}{-}} 
  \FunctionTok{bootstrap\_model}\NormalTok{(}
    \AttributeTok{saved\_configural =}\NormalTok{ results.rs}\SpecialCharTok{$}\NormalTok{model\_configural,}
    \AttributeTok{data =}\NormalTok{ DF,}
    \AttributeTok{model =}\NormalTok{ model.rs, }
    \AttributeTok{group =} \StringTok{"sample"}\NormalTok{, }
    \AttributeTok{nboot =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{invariance\_index =} \StringTok{"cfi"}\NormalTok{,}
    \AttributeTok{invariance\_rule =}\NormalTok{ .}\DecValTok{01}\NormalTok{, }
    \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{, }\StringTok{"residuals"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\normalsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.model.rs}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
## # A tibble: 2 x 7
##   model      non_invariant random_non_invariant  h_nmi    h_mi h_nmi_p  h_mi_p
##   <chr>              <dbl>                <dbl>  <dbl>   <dbl>   <dbl>   <dbl>
## 1 intercepts         0.998                    0 3.05   -3.05    0.972  -0.972 
## 2 residuals          0.002                    0 0.0895 -0.0895  0.0285 -0.0285
\end{verbatim}

Next, we would examine the strength of the effects of replication on each parameter at the intercept level. By examining Table \ref{tab:table-boot-effects}, it is clear that most of the item means are unlikely to replicate, even though two particular items can be used to create partial invariance. Figures \ref{fig:rs-partial-pic} and \ref{fig:rs-partial-pic2} display the three plots provided in the \texttt{bootstrap\_partial()} function. In general, we should expect \(M_{D}\) = 0.23 when items are invariant and \(M_{D}\) = 0.26 when items are not invariant. The effect size of non-invariant items ranges from 0.43 to 0.62.

The density plot shown at the bottom of Figure \ref{fig:rs-partial-pic} illustrates the likely reasons for the differences found in the top plots. It appears that many items show a bimodal distribution within group 1 (Clinical Sample) and when items are invariant, the intercept averages to the same intercept as group 2 (Student Sample). In non-invariant estimates, the same bimodal distributions are found, but they are more extreme than the student samples, and therefore, item show different averages due to the presence of two separate means of data. Further, some items also appear to show two separate student item averages within the data. This result suggests that it would be fruitful to understand a potential predictor of these differences or other confounding variable that separates these samples, creating differences in item averages.

In summary, if one were planning a replication, the prediction would be that item intercepts would likely not replicate, with a large effect size (i.e., it is easy to judge \(h_{nmi_p}\) close to the max of one as large). While this study found partial invariance by relaxing constraints on two individual items, bootstrapped partial invariance indicates that any item could potentially be problematic with an effect size averaging \(d\) \textasciitilde{} 0.50 difference in means. While \(d_{MACS}\) values represented a ``small'' effect based on previous publications, this effect may be muted by examining both loadings and intercepts. The results here suggest that the effect is driven by intercepts. The overall average score on items is high: \(M_M\) = 5.04 (\(M_{SD}\) = 1.72). Given the mean standard deviation, a \(d\) \textasciitilde{} 0.50 represents 0.86 or nearly one whole point on the scale. A researcher could decide that at least \(d\) = 0.33 or at least a third of a standard deviation would be an important change and set that as their smallest effect size of interest for invariance. Further, a newly planned study should investigate what variables may predict when and why samples separate into bimodal representations for item means.

\normalsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.partial.rs }\OtherTok{\textless{}{-}} 
  \FunctionTok{bootstrap\_partial}\NormalTok{(}
    \AttributeTok{saved\_model =}\NormalTok{ results.rs}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.intercepts,}
    \AttributeTok{data =}\NormalTok{ DF,}
    \AttributeTok{model =}\NormalTok{ model.rs, }
    \AttributeTok{group =} \StringTok{"sample"}\NormalTok{, }
    \AttributeTok{nboot =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{invariance\_index =} \StringTok{"cfi"}\NormalTok{,}
    \AttributeTok{invariance\_rule =}\NormalTok{ .}\DecValTok{01}\NormalTok{, }
    \AttributeTok{invariance\_compare =} \FunctionTok{fitmeasures}\NormalTok{(results.rs}\SpecialCharTok{$}\NormalTok{invariance\_models}\SpecialCharTok{$}\NormalTok{model.loadings, }\StringTok{"cfi"}\NormalTok{), }
    \AttributeTok{partial\_step =} \StringTok{"intercepts"}\NormalTok{,}
    \AttributeTok{group.equal =} \FunctionTok{c}\NormalTok{(}\StringTok{"loadings"}\NormalTok{, }\StringTok{"intercepts"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:table-boot-effects}Boot Partial Effects Results for RS-14 Intercepts}

\begin{tabular}{lllll}
\toprule
Term & Non-Invariant & Random Non-Invariant & $h_{nmi}$ & $h_{nmi_p}$\\
\midrule
RS  Intercept & 0.991 & 0.007 & 2.784 & 0.886\\
RS1  Intercept & 0.989 & 0.007 & 2.764 & 0.880\\
RS10  Intercept & 0.988 & 0.007 & 2.755 & 0.877\\
RS11  Intercept & 0.991 & 0.007 & 2.784 & 0.886\\
RS12  Intercept & 0.991 & 0.007 & 2.784 & 0.886\\
RS13  Intercept & 0.991 & 0.007 & 2.784 & 0.886\\
RS14  Intercept & 0.990 & 0.007 & 2.774 & 0.883\\
RS2  Intercept & 0.985 & 0.007 & 2.728 & 0.869\\
RS3  Intercept & 0.988 & 0.007 & 2.755 & 0.877\\
RS4  Intercept & 0.990 & 0.007 & 2.774 & 0.883\\
RS5  Intercept & 0.984 & 0.007 & 2.720 & 0.866\\
RS6  Intercept & 0.979 & 0.007 & 2.683 & 0.854\\
RS7  Intercept & 0.974 & 0.007 & 2.650 & 0.844\\
RS8  Intercept & 0.991 & 0.007 & 2.784 & 0.886\\
RS9  Intercept & 0.987 & 0.007 & 2.746 & 0.874\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs-partial-pic-1.pdf}
\caption{\label{fig:rs-partial-pic}RS-14 scale invariance for item intercepts. The left panel indicates the raw score difference between groups and items, while the right panel indicates the effect size for group differences based on invariance.}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs-partial-pic2-1.pdf}
\caption{\label{fig:rs-partial-pic2}RS-14 scale invariance density plots, illustrating invariant versus non-invariant bootstrapped and random runs for each parameter.}
\end{figure}

\hypertarget{an-example-extension}{%
\subsection{An Example Extension}\label{an-example-extension}}

One benefit of the open science movement on scale development is the publication of datasets or covariance tables with published articles. We can extend our examination of potential replication on other variables that may effect assessment of underlying phenomena. For example, scale translation across languages is not only impacted by the literal conversion of concepts from one language to another, but also the cultural and societal norms of the target population (Cha et al., 2007; Chang et al., 1999; Yu et al., 2004). The RS14 was tested in Chinese speaking samples in Chen et al. (2020) across five different large samples and determined that the scale showed good psychometric properties for use within Chinese speaking samples as a one-factor model of resiliency. Given these results, another researcher may assume that the models would be comparable between English speaking (i.e., likely United States) and Chinese speaking samples. With the published data, we can use \texttt{visualizemi} to determine if the RS14 is invariant across language/culture, and if the results would likely replicate if tested on a new set of samples, and what, if any, effect size differences are found in parameters. The code used for this analysis is presented in Appendix 7.

The Aiena et al. (2014) data used in our previous example was first filtered to only college students, as we have already noted that clinical samples show slightly different intercepts for at least two of the items from student samples. The Chen et al. (2020) data also included college students, which allows us to test a comparable sample that varies on translation and culture. The test of measurement invariance indicated that the factor structure and loadings were invariant across groups. Much like our test of clinical versus student samples, the results indicated that the intercepts were not consistent across groups. Within the English clinical/student sample, partial invariance could be achieved by relaxing two item intercept constraints (item 6 and 7). To achieve ``partial'' invariance between the Chinese and English speaking samples, we would need to relax more than half of the items (specifically, eight items: 1, 2, 3, 4, 10, 11, 12, 14), and it would be difficult to suggest partial invariance given this finding. \(d_{MACS}\) values range from 0.26 to 0.32 for the eight items which could be interpreted a small to medium given Nye et al. (2019)'s simulation study. Figure \ref{fig:rs14-chinese-pic} portrays the results from the second item on the RS14 (\emph{life accomplishments}).

The results of model bootstrapping indicated that the effect of the loadings was likely to replicate (only invariant results were found), but the intercepts were never found to be invariant compared to a randomized sample (\(h_{nmi_p}\) = 1). Therefore, we would expect that these differences are persistent, either due to the adaptation or cultural differences in resiliency across samples. The bootstrapped partial invariance demonstrates that each item intercept has a medium to large difference between the two samples as shown in Figure \ref{fig:rs14-chinese-effect-pic}, which may explain why full or partial invariance cannot be achieved. This result does not invalidate either version of the scale, but informs researchers of potential baseline differences in item responding for the two samples. Therefore, careful interpretation should be made when comparing these two samples in other instances, as differences in latent means may be the default finding, but with these results one may determine if their results are different from expectations of general scale responding.

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs14-chinese-pic-1.pdf}
\caption{\label{fig:rs14-chinese-pic}The differences in intercepts for the second item on the RS14 by language sample. The differences between intercepts are shown on the left hand side with a clear separation between lines. The latent means also show a clear difference between groups where the English group appears to have lower scores overall than the Chinese group.}
\end{figure}

\begin{figure}
\centering
\includegraphics{manuscript_files/figure-latex/rs14-chinese-effect-pic-1.pdf}
\caption{\label{fig:rs14-chinese-effect-pic}Effect size differences for item intercepts on the English versus Chinese samples for the RS14.}
\end{figure}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In this tutorial, we examined how to use multiple tools to examine measurement invariance and its potential replication. Model fit comparisons and statistics can be paired with the proposed effect size measures, and a visualization to examine individual items and the overall latent mean scores. The impact of potential replication was estimated on the overall model and the individual parameters. Using real data, the effect of two non-invariant item intercepts was examined and visualized. This tutorial manuscript has provided a concrete way to plan for pre-registration and/or registered reports. Researchers could simulate results based on published or previously collected data to determine the likelihood and size of potential replication. They could plan and pre-register a smallest effect of interest. For example, we may determine that an \(h_{nmi_p}\) value above .20 represents an important level of non-invariance for our model overall, while \(h_{nmi_p}\) \textgreater{} .30 for any individual parameter warrant caution against invariance for groups. Others have begun to discuss the importance of focusing on effects in the scale of the data and their practical importance (Anvari \& Lakens, 2021; Cumming, 2012).

From the example, our interpretation may be that the difference between group's latent means is large, as a 0.72 change on a 7 point scale is approximately 10\% more resiliency for students when compared to the clinical sample. Practically, 10\% in resiliency for an area of the United States (Mississippi) often hit with natural disasters (hurricanes, tornadoes, floods) and high levels of poverty would be very important. Even the smaller difference of .5 point on each individual item could translate into increases in resiliency, and these results may elucidate avenues for further exploration into areas of focus within resiliency, given the items. The secondary example showed that we can extend these results to other samples to examine other potentially impactful variables on assessment. The findings replicate in the sense that the scale shows the same invariant issues with intercepts on a Chinese versus English sample comparison. However, in this analysis, it is clear that the differences in items averages are much larger and across all items, rather than a few.

What do the results of a study on measurement invariance with these results tell us about replication, generalizability, and validity overall? If a researcher decides their effects are large, they should likely caution against suggesting that these scores are directly comparable without weighting or other adjustment. Let's consider a scenario wherein the change metric between models picked (i.e., \(\Delta\)CFI, \(\Delta\)RMSEA) indicates a ``significant'' change in model fit. However, if both the effect size and a visual inspection of the invariance indicates a small difference, we may decide to lessen the practical importance of those results, much like ``just significant'' \emph{p}-values with small effect sizes are treated now. The results from our Chinese versus English comparison show us the other scenario: non-invariant results that clearly indicate differences with a large effect size on both replication and item average differences. Given that the goal of measurement invariance is to compare \emph{estimates}, we should expect some differences across samples due to the nature of sampling and estimation. It may be that many of the published models presented represent these effects - small variations between groups due to sampling error or other small crud - but do not represent a fundamental problem with the measurement or generalizability of the results. The \texttt{visualizemi} package is one useful tool to help sort out if findings are this small sampling error or differences in samples due to other variables.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-aiena2014}{}}%
Aiena, B. J., Baczwaski, B. J., Schulenberg, S. E., \& Buchanan, E. M. (2014). Measuring Resilience With the RS{\textendash}14: A Tale of Two Samples. \emph{Journal of Personality Assessment}, \emph{97}(3), 291--300. \url{https://doi.org/10.1080/00223891.2014.951445}

\leavevmode\vadjust pre{\hypertarget{ref-akaike1998}{}}%
Akaike, H. (1998). \emph{Information theory and an extension of the maximum likelihood principle} (E. Parzen, K. Tanabe, \& G. Kitagawa, Eds.; pp. 199--213). Springer New York. \url{http://link.springer.com/10.1007/978-1-4612-1694-0_15}

\leavevmode\vadjust pre{\hypertarget{ref-anvari2021}{}}%
Anvari, F., \& Lakens, D. (2021). Using anchor-based methods to determine the smallest effect size of interest. \emph{Journal of Experimental Social Psychology}, \emph{96}, 104159. \url{https://doi.org/10.1016/j.jesp.2021.104159}

\leavevmode\vadjust pre{\hypertarget{ref-barry2014}{}}%
Barry, A. E., Chaney, B., Piazza-Gardner, A. K., \& Chavarria, E. A. (2014). Validity and Reliability Reporting Practices in the Field of Health Education and Behavior: A Review of Seven Journals. \emph{Health Education \& Behavior}, \emph{41}(1), 12--18. \url{https://doi.org/10.1177/1090198113483139}

\leavevmode\vadjust pre{\hypertarget{ref-beaujean2014}{}}%
Beaujean, A. A. (2014). \emph{Latent variable modeling using r: A step by step guide}. Routledge/Taylor \& Francis Group.

\leavevmode\vadjust pre{\hypertarget{ref-bentler1990}{}}%
Bentler, P. M. (1990). Comparative fit indexes in structural models. \emph{Psychological Bulletin}, \emph{107}(2), 238--246. \url{https://doi.org/10.1037/0033-2909.107.2.238}

\leavevmode\vadjust pre{\hypertarget{ref-bentler1995}{}}%
Bentler, P. M. (1995). \emph{EQS structural equations program manual}.

\leavevmode\vadjust pre{\hypertarget{ref-boker2011}{}}%
Boker, S., Neale, M., Maes, H., Wilde, M., Spiegel, M., Brick, T., Spies, J., Estabrook, R., Kenny, S., Bates, T., Mehta, P., \& Fox, J. (2011). OpenMx: An Open Source Extended Structural Equation Modeling Framework. \emph{Psychometrika}, \emph{76}(2), 306--317. \url{https://doi.org/10.1007/s11336-010-9200-6}

\leavevmode\vadjust pre{\hypertarget{ref-brown2015}{}}%
Brown, T. A. (2015). \emph{Confirmatory factor analysis for applied research} (Second edition). The Guilford Press.

\leavevmode\vadjust pre{\hypertarget{ref-byrne1989}{}}%
Byrne, B. M., Shavelson, R. J., \& Muthén, B. (1989). Testing for the equivalence of factor covariance and mean structures: The issue of partial measurement invariance. \emph{Psychological Bulletin}, \emph{105}(3), 456--466. \url{https://doi.org/10.1037/0033-2909.105.3.456}

\leavevmode\vadjust pre{\hypertarget{ref-cao2022}{}}%
Cao, C., \& Liang, X. (2022). The impact of model size on the sensitivity of fit measures in measurement invariance testing. \emph{Structural Equation Modeling: A Multidisciplinary Journal}, \emph{29}(5), 744--754. \url{https://doi.org/10.1080/10705511.2022.2056893}

\leavevmode\vadjust pre{\hypertarget{ref-cha2007}{}}%
Cha, E.-S., Kim, K. H., \& Erlen, J. A. (2007). Translation of scales in cross{-}cultural research: issues and techniques. \emph{Journal of Advanced Nursing}, \emph{58}(4), 386--395. \url{https://doi.org/10.1111/j.1365-2648.2007.04242.x}

\leavevmode\vadjust pre{\hypertarget{ref-chang1999}{}}%
Chang, A. M., Chau, J. P. C., \& Holroyd, E. (1999). Translation of questionnaires and issues of equivalence. \emph{Journal of Advanced Nursing}, \emph{29}(2), 316--322. \url{https://doi.org/10.1046/j.1365-2648.1999.00891.x}

\leavevmode\vadjust pre{\hypertarget{ref-chen2020}{}}%
Chen, W., Xie, E., Tian, X., \& Zhang, G. (2020). Psychometric properties of the Chinese version of the Resilience Scale (RS-14): Preliminary results. \emph{PLOS ONE}, \emph{15}(10), e0241606. \url{https://doi.org/10.1371/journal.pone.0241606}

\leavevmode\vadjust pre{\hypertarget{ref-cheung2002}{}}%
Cheung, G. W., \& Rensvold, R. B. (2002). Evaluating Goodness-of-Fit Indexes for Testing Measurement Invariance. \emph{Structural Equation Modeling: A Multidisciplinary Journal}, \emph{9}(2), 233--255. \url{https://doi.org/10.1207/s15328007sem0902_5}

\leavevmode\vadjust pre{\hypertarget{ref-chorpita2000}{}}%
Chorpita, B. F., Yim, L., Moffitt, C., Umemoto, L. A., \& Francis, S. E. (2000). Assessment of symptoms of DSM-IV anxiety and depression in children: a revised child anxiety and depression scale. \emph{Behaviour Research and Therapy}, \emph{38}(8), 835--855. \url{https://doi.org/10.1016/S0005-7967(99)00130-8}

\leavevmode\vadjust pre{\hypertarget{ref-cohen2003}{}}%
Cohen, J., Cohen, P., West, S. G., \& Aiken, L. (2003). \emph{Applied multiple regression / correlation analysis for the behavioral sciences} (3rd ed.). Lawrence Erlbaum Associates.

\leavevmode\vadjust pre{\hypertarget{ref-counsell2020}{}}%
Counsell, A., Cribbie, R. A., \& Flora, D. B. (2020). Evaluating equivalence testing methods for measurement invariance. \emph{Multivariate Behavioral Research}, \emph{55}(2), 312--328. \url{https://doi.org/10.1080/00273171.2019.1633617}

\leavevmode\vadjust pre{\hypertarget{ref-cumming2012}{}}%
Cumming, G. (2012). \emph{Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-devellis2022}{}}%
DeVellis, R. F., \& Thorpe, C. T. (2022). \emph{Scale development: Theory and applications} (Fifth edition). SAGE Publications, Inc.

\leavevmode\vadjust pre{\hypertarget{ref-dueber2023}{}}%
Dueber, D. (2023). \emph{Dmacs}. \url{https://github.com/ddueber/dmacs}

\leavevmode\vadjust pre{\hypertarget{ref-epskamp2022}{}}%
Epskamp, S. (2022). \emph{semPlot: Path diagrams and visual analysis of various SEM packages' output}. \url{https://CRAN.R-project.org/package=semPlot}

\leavevmode\vadjust pre{\hypertarget{ref-flake2020}{}}%
Flake, J. K., \& Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. \emph{Advances in Methods and Practices in Psychological Science}, \emph{3}(4), 456--465. \url{https://doi.org/10.1177/2515245920952393}

\leavevmode\vadjust pre{\hypertarget{ref-hobson2019}{}}%
Hobson, H. (2019). Registered reports are an ally to early career researchers. \emph{Nature Human Behaviour}, \emph{3}(10), 1010--1010. \url{https://doi.org/10.1038/s41562-019-0701-8}

\leavevmode\vadjust pre{\hypertarget{ref-jin2020}{}}%
Jin, Y. (2020). A note on the cutoff values of alternative fit indices to evaluate measurement invariance for ESEM models. \emph{International Journal of Behavioral Development}, \emph{44}(2), 166--174. \url{https://doi.org/10.1177/0165025419866911}

\leavevmode\vadjust pre{\hypertarget{ref-juxf6reskog1971}{}}%
Jöreskog, K. G. (1971). Simultaneous factor analysis in several populations. \emph{Psychometrika}, \emph{36}(4), 409--426. \url{https://doi.org/10.1007/BF02291366}

\leavevmode\vadjust pre{\hypertarget{ref-juxf6reskog2001}{}}%
Jöreskog, K. G., \& Sörbom, D. (2001). \emph{LISREL 8: user's reference guide} (2. ed., updated to LISREL 8). SSI Scientific Software Internat.

\leavevmode\vadjust pre{\hypertarget{ref-lakens2017}{}}%
Lakens, D. (2017). Equivalence Tests. \emph{Social Psychological and Personality Science}, \emph{8}(4), 355--362. \url{https://doi.org/10.1177/1948550617697177}

\leavevmode\vadjust pre{\hypertarget{ref-makel2014}{}}%
Makel, M. C., \& Plucker, J. A. (2014). Facts Are More Important Than Novelty: Replication in the Education Sciences. \emph{Educational Researcher}, \emph{43}(6), 304--316. \url{https://doi.org/10.3102/0013189X14545513}

\leavevmode\vadjust pre{\hypertarget{ref-makel2012}{}}%
Makel, M. C., Plucker, J. A., \& Hegarty, B. (2012). Replications in Psychology Research: How Often Do They Really Occur? \emph{Perspectives on Psychological Science}, \emph{7}(6), 537--542. \url{https://doi.org/10.1177/1745691612460688}

\leavevmode\vadjust pre{\hypertarget{ref-marsh2004}{}}%
Marsh, H. W., Hau, K.-T., \& Wen, Z. (2004). In search of golden rules: Comment on hypothesis-testing approaches to setting cutoff values for fit indexes and dangers in overgeneralizing hu and bentler's (1999) findings. \emph{Structural Equation Modeling: A Multidisciplinary Journal}, \emph{11}(3), 320--341. \url{https://doi.org/10.1207/s15328007sem1103_2}

\leavevmode\vadjust pre{\hypertarget{ref-mayo-wilson2021}{}}%
Mayo-Wilson, E., Grant, S., Supplee, L., Kianersi, S., Amin, A., DeHaven, A., \& Mellor, D. (2021). Evaluating implementation of the transparency and openness promotion~(TOP) guidelines:~The TRUST process for rating journal policies, procedures, and practices. \emph{Research Integrity and Peer Review}, \emph{6}(1), 9. \url{https://doi.org/10.1186/s41073-021-00112-8}

\leavevmode\vadjust pre{\hypertarget{ref-meredith1993}{}}%
Meredith, W. (1993). Measurement invariance, factor analysis and factorial invariance. \emph{Psychometrika}, \emph{58}(4), 525--543. \url{https://doi.org/10.1007/BF02294825}

\leavevmode\vadjust pre{\hypertarget{ref-nelson2018}{}}%
Nelson, L. D., Simmons, J., \& Simonsohn, U. (2018). Psychology's renaissance. \emph{Annual Review of Psychology}, \emph{69}(1), 511--534. \url{https://doi.org/10.1146/annurev-psych-122216-011836}

\leavevmode\vadjust pre{\hypertarget{ref-nosek2018}{}}%
Nosek, B. A., Ebersole, C. R., DeHaven, A. C., \& Mellor, D. T. (2018). The preregistration revolution. \emph{Proceedings of the National Academy of Sciences of the United States of America}, \emph{115}(11), 2600--2606. \url{https://doi.org/10.1073/pnas.1708274114}

\leavevmode\vadjust pre{\hypertarget{ref-nosek2014}{}}%
Nosek, B. A., \& Lakens, D. (2014). Registered Reports: A Method to Increase the Credibility of Published Results. \emph{Social Psychology}, \emph{45}(3), 137--141. \url{https://doi.org/10.1027/1864-9335/a000192}

\leavevmode\vadjust pre{\hypertarget{ref-nye2019}{}}%
Nye, C. D., Bradburn, J., Olenick, J., Bialko, C., \& Drasgow, F. (2019). How Big Are My Effects? Examining the Magnitude of Effect Sizes in Studies of Measurement Equivalence. \emph{Organizational Research Methods}, \emph{22}(3), 678--709. \url{https://doi.org/10.1177/1094428118761122}

\leavevmode\vadjust pre{\hypertarget{ref-nye2011}{}}%
Nye, C. D., \& Drasgow, F. (2011). Effect size indices for analyses of measurement equivalence: Understanding the practical importance of differences between groups. \emph{Journal of Applied Psychology}, \emph{96}(5), 966--980. \url{https://doi.org/10.1037/a0022955}

\leavevmode\vadjust pre{\hypertarget{ref-putnick2016}{}}%
Putnick, D. L., \& Bornstein, M. H. (2016). Measurement invariance conventions and reporting: The state of the art and future directions for psychological research. \emph{Developmental Review}, \emph{41}, 71--90. \url{https://doi.org/10.1016/j.dr.2016.06.004}

\leavevmode\vadjust pre{\hypertarget{ref-robinson2023}{}}%
Robinson, D., Hayes, A., Couch {[}aut, S., cre, Software, P., PBC, Patil, I., Chiu, D., Gomez, M., Demeshev, B., Menne, D., Nutter, B., Johnston, L., Bolker, B., Briatte, F., Arnold, J., Gabry, J., Selzer, L., Simpson, G., \ldots{} Reinhart, A. (2023). \emph{Broom: Convert statistical objects into tidy tibbles}. \url{https://CRAN.R-project.org/package=broom}

\leavevmode\vadjust pre{\hypertarget{ref-rosseel2012}{}}%
Rosseel, Y. (2012). Lavaan: An r package for structural equation modeling. \emph{Journal of Statistical Software}, \emph{48}(1), 1--36. \url{https://doi.org/10.18637/jss.v048.i02}

\leavevmode\vadjust pre{\hypertarget{ref-schwarz1978}{}}%
Schwarz, G. (1978). Estimating the dimension of a model. \emph{The Annals of Statistics}, \emph{6}(2), 461--464. \url{https://www.jstor.org/stable/2958889}

\leavevmode\vadjust pre{\hypertarget{ref-shadish2001}{}}%
Shadish, W. R., Cook, T. D., \& Campbell, D. T. (2001). \emph{Experimental and quasi-experimental designs for generalized causal inference}. Houghton Mifflin.

\leavevmode\vadjust pre{\hypertarget{ref-suxf6rbom1978}{}}%
Sörbom, D. (1978). An alternative to the methodology for analysis of covariance. \emph{Psychometrika}, \emph{43}(3), 381--396. \url{https://doi.org/10.1007/BF02293647}

\leavevmode\vadjust pre{\hypertarget{ref-stark2006}{}}%
Stark, S., Chernyshenko, O. S., \& Drasgow, F. (2006). Detecting differential item functioning with confirmatory factor analysis and item response theory: Toward a unified strategy. \emph{Journal of Applied Psychology}, \emph{91}(6), 1292--1306. \url{https://doi.org/10.1037/0021-9010.91.6.1292}

\leavevmode\vadjust pre{\hypertarget{ref-steiger1990}{}}%
Steiger, J. H. (1990). Structural model evaluation and modification: An interval estimation approach. \emph{Multivariate Behavioral Research}, \emph{25}(2), 173--180. \url{https://doi.org/10.1207/s15327906mbr2502_4}

\leavevmode\vadjust pre{\hypertarget{ref-trent2013}{}}%
Trent, L. R., Buchanan, E., Ebesutani, C., Ale, C. M., Heiden, L., Hight, T. L., Damon, J. D., \& Young, J. (2013). A measurement invariance examination of the revised child anxiety and depression scale in a southern sample: Differential item functioning between african american and caucasian youth. \emph{Assessment}, \emph{20}(2), 175--187. \url{https://doi.org/10.1177/1073191112450907}

\leavevmode\vadjust pre{\hypertarget{ref-tucker1973}{}}%
Tucker, L. R., \& Lewis, C. (1973). A reliability coefficient for maximum likelihood factor analysis. \emph{Psychometrika}, \emph{38}(1), 1--10. \url{https://doi.org/10.1007/BF02291170}

\leavevmode\vadjust pre{\hypertarget{ref-vandeschoot2015}{}}%
Van De Schoot, R., Schmidt, P., De Beuckelaer, A., Lek, K., \& Zondervan-Zwijnenburg, M. (2015). Editorial: Measurement invariance. \emph{Frontiers in Psychology}, \emph{6}. \url{https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01064}

\leavevmode\vadjust pre{\hypertarget{ref-vazire2022}{}}%
Vazire, S., Schiavone, S. R., \& Bottesini, J. G. (2022). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. \emph{Current Directions in Psychological Science}, \emph{31}(2), 162--168. \url{https://doi.org/10.1177/09637214211067779}

\leavevmode\vadjust pre{\hypertarget{ref-wagnild2009}{}}%
Wagnild, G. (2009). A review of the resilience scale. \emph{Journal of Nursing Measurement}, \emph{17}(2), 105--113. \url{https://doi.org/10.1891/1061-3749.17.2.105}

\leavevmode\vadjust pre{\hypertarget{ref-weidman2017}{}}%
Weidman, A. C., Steckler, C. M., \& Tracy, J. L. (2017). The jingle and jangle of emotion assessment: Imprecise measurement, casual scale usage, and conceptual fuzziness in emotion research. \emph{Emotion}, \emph{17}(2), 267--295. \url{https://doi.org/10.1037/emo0000226}

\leavevmode\vadjust pre{\hypertarget{ref-R-ggplot2}{}}%
Wickham, H. (2016). \emph{ggplot2: Elegant graphics for data analysis}. Springer-Verlag New York. \url{https://ggplot2.tidyverse.org}

\leavevmode\vadjust pre{\hypertarget{ref-wilke2020}{}}%
Wilke, C. O. (2020). \emph{Cowplot: Streamlined plot theme and plot annotations for 'ggplot2'}. \url{https://CRAN.R-project.org/package=cowplot}

\leavevmode\vadjust pre{\hypertarget{ref-yu2004}{}}%
Yu, D. S. F., Lee, D. T. F., \& Woo, J. (2004). Issues and Challenges of Instrument Translation. \emph{Western Journal of Nursing Research}, \emph{26}(3), 307--320. \url{https://doi.org/10.1177/0193945903260554}

\leavevmode\vadjust pre{\hypertarget{ref-zwaan2018}{}}%
Zwaan, R. A., Etz, A., Lucas, R. E., \& Donnellan, M. B. (2018). Making replication mainstream. \emph{Behavioral and Brain Sciences}, \emph{41}, e120. \url{https://doi.org/10.1017/S0140525X17001972}

\end{CSLReferences}

\newpage

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{simulating-from-models}{%
\section{Simulating from models}\label{simulating-from-models}}

Here's an example of how to simulate directly from a \texttt{lavaan} model:

\small

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first build your model}
\CommentTok{\# this example is separate for each group}
\NormalTok{model.invariant.g1 }\OtherTok{\textless{}{-}} \StringTok{"}
\StringTok{\# loadings}
\StringTok{lv =\textasciitilde{} .8*q1 + .4*q2 + .6*q3 + .3*q4 + .6*q5 }
\StringTok{\# set the residual for invariance on q4}
\StringTok{q4 \textasciitilde{}\textasciitilde{} 1*q4}
\StringTok{\# set the intercept for invariance on q4}
\StringTok{q4 \textasciitilde{} 0*1}
\StringTok{\# set the intercept to zero for df purposes}
\StringTok{q1 \textasciitilde{} 0*1}
\StringTok{\# allow the latent mean to be estimated }
\StringTok{lv \textasciitilde{} 1"}
\NormalTok{model.invariant.g2 }\OtherTok{\textless{}{-}} \StringTok{"lv =\textasciitilde{} .77*q1 + .43*q2 + .58*q3 + .3*q4 + .61*q5}
\StringTok{q4 \textasciitilde{}\textasciitilde{} 1*q4}
\StringTok{q4 \textasciitilde{} 0*1}
\StringTok{q1 \textasciitilde{} 0*1}
\StringTok{lv \textasciitilde{} 1"}

\CommentTok{\# simulate data invariant separately for each group}
\NormalTok{df.invariant }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \CommentTok{\# lavaan function }
  \FunctionTok{simulateData}\NormalTok{(}
    \CommentTok{\# model with estimates }
    \AttributeTok{model =}\NormalTok{ model.invariant.g1, }
    \CommentTok{\# how many data points}
    \AttributeTok{sample.nobs =} \DecValTok{250}\NormalTok{, }
    \CommentTok{\# mean structure for mgcfa models }
    \AttributeTok{meanstructure =}\NormalTok{ T, }
    \CommentTok{\# model type}
    \AttributeTok{model.type =} \StringTok{"cfa"}\NormalTok{,}
    \CommentTok{\# set seed for reproducibility }
    \AttributeTok{seed =} \DecValTok{1234}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# add a group label to the data }
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \StringTok{"Group 1"}\NormalTok{), }
  \FunctionTok{simulateData}\NormalTok{(}
    \AttributeTok{model =}\NormalTok{ model.invariant.g2, }
    \AttributeTok{sample.nobs =} \DecValTok{250}\NormalTok{, }
    \AttributeTok{meanstructure =}\NormalTok{ T, }
    \AttributeTok{model.type =} \StringTok{"cfa"}\NormalTok{,}
    \AttributeTok{seed =} \DecValTok{1234}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \StringTok{"Group 2"}\NormalTok{) }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

\newpage

\hypertarget{simulating-from-matrices}{%
\section{Simulating from matrices}\label{simulating-from-matrices}}

Here's an example of how to simulate using \texttt{MASS} and covariance or correlation matrices.

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}

\CommentTok{\# covariance matrix}
\NormalTok{university.cov }\OtherTok{\textless{}{-}} \FunctionTok{lav\_matrix\_lower2full}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\FloatTok{169.00}\NormalTok{, }
      \FloatTok{73.710}\NormalTok{, }\FloatTok{182.2500}\NormalTok{,}
      \FloatTok{73.229}\NormalTok{, }\FloatTok{88.4250}\NormalTok{, }\FloatTok{171.6100}\NormalTok{,}
      \FloatTok{63.375}\NormalTok{, }\FloatTok{72.5625}\NormalTok{, }\FloatTok{127.7250}\NormalTok{, }\FloatTok{156.2500}\NormalTok{,}
      \FloatTok{42.120}\NormalTok{, }\FloatTok{67.4325}\NormalTok{, }\FloatTok{122.0265}\NormalTok{, }\FloatTok{123.1875}\NormalTok{, }\FloatTok{182.2500}\NormalTok{,}
      \FloatTok{57.226}\NormalTok{, }\FloatTok{63.2610}\NormalTok{, }\FloatTok{117.1926}\NormalTok{, }\FloatTok{154.4250}\NormalTok{, }\FloatTok{138.0240}\NormalTok{, }\FloatTok{201.6400}\NormalTok{,}
      \FloatTok{30.875}\NormalTok{, }\FloatTok{32.0625}\NormalTok{, }\FloatTok{60.9805}\NormalTok{, }\FloatTok{62.9375}\NormalTok{, }\FloatTok{76.9500}\NormalTok{, }\FloatTok{79.5910}\NormalTok{, }\FloatTok{90.2500}\NormalTok{,}
      \FloatTok{36.075}\NormalTok{, }\FloatTok{38.9610}\NormalTok{, }\FloatTok{61.0722}\NormalTok{, }\FloatTok{58.2750}\NormalTok{, }\FloatTok{65.9340}\NormalTok{, }\FloatTok{70.9290}\NormalTok{, }\FloatTok{81.1965}\NormalTok{, }\FloatTok{123.2100}\NormalTok{,}
      \FloatTok{18.096}\NormalTok{, }\FloatTok{21.1410}\NormalTok{, }\FloatTok{26.2131}\NormalTok{, }\FloatTok{39.1500}\NormalTok{, }\FloatTok{44.6310}\NormalTok{, }\FloatTok{46.9452}\NormalTok{, }\FloatTok{48.7635}\NormalTok{, }\FloatTok{56.0106}\NormalTok{, }\FloatTok{75.6900}\NormalTok{))}

\CommentTok{\# give it names}
\FunctionTok{rownames}\NormalTok{(university.cov) }\OtherTok{\textless{}{-}}
    \FunctionTok{colnames}\NormalTok{(university.cov) }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\StringTok{"class"}\NormalTok{, }\StringTok{"social"}\NormalTok{, }\StringTok{"learn"}\NormalTok{, }\StringTok{"chronic"}\NormalTok{, }\StringTok{"physical"}\NormalTok{, }\StringTok{"sex"}\NormalTok{, }
      \StringTok{"depression"}\NormalTok{, }\StringTok{"anxiety"}\NormalTok{, }\StringTok{"stress"}\NormalTok{)}

\CommentTok{\# means {-} you need standard deviation if you only have a correlation matrix }
\NormalTok{university.means }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{3.4}\NormalTok{, }\FloatTok{4.3}\NormalTok{, }\FloatTok{3.7}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{4.5}\NormalTok{, }\FloatTok{1.2}\NormalTok{, }\FloatTok{4.0}\NormalTok{, }\FloatTok{3.5}\NormalTok{, }\FloatTok{4.2}\NormalTok{)}

\CommentTok{\# use mass function}
\NormalTok{DF }\OtherTok{\textless{}{-}} \FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ university.means, }\AttributeTok{Sigma =}\NormalTok{ university.cov)}

\FunctionTok{head}\NormalTok{(DF)}
\end{Highlighting}
\end{Shaded}

\normalsize

\begin{verbatim}
##          class      social     learn   chronic    physical         sex
## [1,] 12.085294 26.83043663 13.895103 20.634374  16.7903295  30.5468832
## [2,]  3.939545 10.93624431  3.093025  8.316128  -7.2199148   9.3579991
## [3,] 15.659627  0.22811723  5.205657  5.224293  -1.4425219  -1.2710662
## [4,] 23.086133 12.43649966  1.891769 -5.913170 -19.4937389 -13.5922410
## [5,] 10.856492 23.30887194 17.124064 11.438840   2.4659294   1.7330709
## [6,] -4.328380  0.07907149 -1.000636 -1.654947  -0.7365838  -0.3958833
##      depression    anxiety      stress
## [1,]  14.174374   8.001766  13.6534279
## [2,]   3.940819  -6.598153  -7.9229552
## [3,]  -8.927508 -10.335571  -2.8100779
## [4,]  -0.149840   6.475669 -15.5300195
## [5,]  -6.492809 -12.524601   0.4924153
## [6,]  11.917631   3.433587   8.1456346
\end{verbatim}

\hypertarget{mgcfa-model-fit-statistics}{%
\section{MGCFA Model Fit Statistics}\label{mgcfa-model-fit-statistics}}

Model fit statistics are provided for each of the ten model combinations (invariant, three sizes for each loadings, intercepts, and residuals). These tables could be used to examine the traditional change in fit statistics cutoff rules of thumb (Cheung \& Rensvold, 2002), such as \(\Delta\) CFI or \(\Delta\) RMSEA, to the visualizations presented in the manuscript.

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab1}Model Fit for Invariant Model}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,516.454 & 7,579.673 & 1.000 & 1.036 & 0.000 & 0.006\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,767.599 & 3,820.421 & 1.000 & 1.008 & 0.000 & 0.021\\
Configural & 7,533.348 & 7,659.786 & 0.991 & 0.982 & 0.030 & 0.026\\
Loadings & 7,528.476 & 7,638.056 & 0.994 & 0.992 & 0.020 & 0.033\\
Intercepts & 7,522.397 & 7,615.118 & 1.000 & 1.003 & 0.000 & 0.035\\
Residuals & 7,520.435 & 7,592.083 & 0.991 & 0.992 & 0.020 & 0.046\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab2}Model Fit for Small Differences in Loadings}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,530.321 & 7,593.540 & 0.977 & 0.955 & 0.049 & 0.025\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,785.242 & 3,838.064 & 0.979 & 0.958 & 0.050 & 0.029\\
Configural & 7,550.991 & 7,677.430 & 0.978 & 0.956 & 0.048 & 0.030\\
Loadings & 7,550.133 & 7,659.713 & 0.966 & 0.952 & 0.051 & 0.047\\
Intercepts & 7,542.675 & 7,635.397 & 0.979 & 0.977 & 0.035 & 0.047\\
Residuals & 7,534.091 & 7,605.739 & 0.993 & 0.994 & 0.019 & 0.054\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab3}Model Fit for Medium Differences in Loadings}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,598.681 & 7,661.900 & 0.946 & 0.893 & 0.078 & 0.035\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,820.090 & 3,872.912 & 0.969 & 0.938 & 0.067 & 0.034\\
Configural & 7,585.839 & 7,712.277 & 0.972 & 0.944 & 0.058 & 0.033\\
Loadings & 7,613.434 & 7,723.014 & 0.865 & 0.807 & 0.107 & 0.079\\
Intercepts & 7,606.648 & 7,699.370 & 0.874 & 0.860 & 0.091 & 0.079\\
Residuals & 7,600.057 & 7,671.705 & 0.880 & 0.895 & 0.079 & 0.091\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab4}Model Fit for Large Differences in Loadings}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,684.982 & 7,748.201 & 0.987 & 0.973 & 0.040 & 0.023\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,857.143 & 3,909.965 & 0.992 & 0.984 & 0.037 & 0.029\\
Configural & 7,622.892 & 7,749.330 & 0.986 & 0.972 & 0.042 & 0.030\\
Loadings & 7,674.188 & 7,783.767 & 0.817 & 0.738 & 0.131 & 0.095\\
Intercepts & 7,667.682 & 7,760.403 & 0.824 & 0.805 & 0.113 & 0.096\\
Residuals & 7,683.177 & 7,754.825 & 0.762 & 0.793 & 0.116 & 0.138\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab5}Model Fit for Small Differences in Intercepts}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,520.471 & 7,583.690 & 1.000 & 1.035 & 0.000 & 0.007\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,767.599 & 3,820.421 & 1.000 & 1.008 & 0.000 & 0.021\\
Configural & 7,533.348 & 7,659.786 & 0.991 & 0.982 & 0.030 & 0.026\\
Loadings & 7,528.476 & 7,638.056 & 0.994 & 0.992 & 0.020 & 0.033\\
Intercepts & 7,526.312 & 7,619.034 & 0.987 & 0.986 & 0.027 & 0.040\\
Residuals & 7,524.356 & 7,596.005 & 0.975 & 0.978 & 0.033 & 0.050\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab6}Model Fit for Medium Differences in Intercepts}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,538.375 & 7,601.594 & 1.000 & 1.033 & 0.000 & 0.008\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,767.599 & 3,820.421 & 1.000 & 1.008 & 0.000 & 0.021\\
Configural & 7,533.348 & 7,659.786 & 0.991 & 0.982 & 0.030 & 0.026\\
Loadings & 7,528.476 & 7,638.056 & 0.994 & 0.992 & 0.020 & 0.033\\
Intercepts & 7,544.002 & 7,636.724 & 0.917 & 0.907 & 0.068 & 0.059\\
Residuals & 7,542.064 & 7,613.712 & 0.905 & 0.917 & 0.065 & 0.067\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab7}Model Fit for Large Differences in Intercepts}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,568.748 & 7,631.967 & 1.000 & 1.032 & 0.000 & 0.008\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,767.599 & 3,820.421 & 1.000 & 1.008 & 0.000 & 0.021\\
Configural & 7,533.348 & 7,659.786 & 0.991 & 0.982 & 0.030 & 0.026\\
Loadings & 7,528.476 & 7,638.056 & 0.994 & 0.992 & 0.020 & 0.033\\
Intercepts & 7,574.054 & 7,666.776 & 0.797 & 0.775 & 0.106 & 0.084\\
Residuals & 7,572.174 & 7,643.823 & 0.785 & 0.813 & 0.097 & 0.090\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab8}Model Fit for Small Differences in Residuals}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,462.007 & 7,525.226 & 1.000 & 1.020 & 0.000 & 0.013\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,703.797 & 3,756.619 & 0.962 & 0.924 & 0.061 & 0.037\\
Configural & 7,469.546 & 7,595.984 & 0.969 & 0.938 & 0.054 & 0.034\\
Loadings & 7,471.637 & 7,581.217 & 0.944 & 0.920 & 0.062 & 0.049\\
Intercepts & 7,465.722 & 7,558.443 & 0.952 & 0.946 & 0.051 & 0.051\\
Residuals & 7,465.986 & 7,537.635 & 0.930 & 0.939 & 0.054 & 0.065\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab9}Model Fit for Medium Differences in Residuals}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,382.013 & 7,445.232 & 0.997 & 0.995 & 0.016 & 0.018\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,602.905 & 3,655.727 & 1.000 & 1.013 & 0.000 & 0.023\\
Configural & 7,368.654 & 7,495.092 & 0.992 & 0.983 & 0.028 & 0.027\\
Loadings & 7,364.904 & 7,474.483 & 0.990 & 0.986 & 0.025 & 0.036\\
Intercepts & 7,358.503 & 7,451.224 & 1.000 & 1.001 & 0.000 & 0.037\\
Residuals & 7,385.958 & 7,457.607 & 0.864 & 0.881 & 0.075 & 0.098\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:tab10}Model Fit for Large Differences in Residuals}

\begin{tabular}{lllllll}
\toprule
Model & AIC & BIC & CFI & TLI & RMSEA & SRMR\\
\midrule
Overall & 7,300.856 & 7,364.075 & 0.998 & 0.995 & 0.015 & 0.018\\
Group Group 1 & 3,765.749 & 3,818.571 & 0.976 & 0.953 & 0.047 & 0.031\\
Group Group 2 & 3,453.099 & 3,505.921 & 0.954 & 0.908 & 0.069 & 0.035\\
Configural & 7,218.848 & 7,345.287 & 0.965 & 0.929 & 0.059 & 0.033\\
Loadings & 7,217.332 & 7,326.912 & 0.955 & 0.935 & 0.056 & 0.045\\
Intercepts & 7,211.566 & 7,304.287 & 0.962 & 0.958 & 0.046 & 0.047\\
Residuals & 7,304.566 & 7,376.215 & 0.562 & 0.619 & 0.137 & 0.189\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{simulated-partial-invariance-results}{%
\section{Simulated Partial Invariance Results}\label{simulated-partial-invariance-results}}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab11}Fit Estimates for Partial Invariance Residuals on Invariant Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}\textasciitilde{} q1 & 0.990 & 0.021\\
q2 \textasciitilde{}\textasciitilde{} q2 & 0.987 & 0.024\\
q3 \textasciitilde{}\textasciitilde{} q3 & 0.996 & 0.014\\
q4 \textasciitilde{}\textasciitilde{} q4 & 1.000 & 0.000\\
q5 \textasciitilde{}\textasciitilde{} q5 & 0.987 & 0.025\\
lv \textasciitilde{}\textasciitilde{} lv & 0.991 & 0.020\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab12}Fit Estimates for Partial Invariance Loadings for Small Loading Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
lv =\textasciitilde{} q1 & 0.993 & 0.019\\
lv =\textasciitilde{} q2 & 0.989 & 0.023\\
lv =\textasciitilde{} q3 & 0.989 & 0.023\\
lv =\textasciitilde{} q4 & 1.000 & 0.000\\
lv =\textasciitilde{} q5 & 0.994 & 0.017\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab13}Fit Estimates for Partial Invariance Loadings for Medium Loading Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
lv =\textasciitilde{} q1 & 0.880 & 0.079\\
lv =\textasciitilde{} q2 & 0.898 & 0.074\\
lv =\textasciitilde{} q3 & 0.878 & 0.081\\
lv =\textasciitilde{} q4 & 0.962 & 0.045\\
lv =\textasciitilde{} q5 & 0.907 & 0.071\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab14}Fit Estimates for Partial Invariance Loadings for Large Loading Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
lv =\textasciitilde{} q1 & 0.762 & 0.116\\
lv =\textasciitilde{} q2 & 0.770 & 0.117\\
lv =\textasciitilde{} q3 & 0.762 & 0.119\\
lv =\textasciitilde{} q4 & 0.971 & 0.041\\
lv =\textasciitilde{} q5 & 0.842 & 0.097\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab15}Fit Estimates for Partial Invariance Loadings for Small Intercept Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}1 & 0.975 & 0.033\\
lv \textasciitilde{}1 & 0.975 & 0.033\\
q2 \textasciitilde{}1 & 0.972 & 0.035\\
q3 \textasciitilde{}1 & 0.972 & 0.036\\
q4 \textasciitilde{}1 & 0.988 & 0.023\\
q5 \textasciitilde{}1 & 0.971 & 0.036\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab16}Fit Estimates for Partial Invariance Loadings for Medium Intercept Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}1 & 0.905 & 0.065\\
lv \textasciitilde{}1 & 0.905 & 0.065\\
q2 \textasciitilde{}1 & 0.901 & 0.067\\
q3 \textasciitilde{}1 & 0.901 & 0.067\\
q4 \textasciitilde{}1 & 0.988 & 0.023\\
q5 \textasciitilde{}1 & 0.902 & 0.067\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab17}Fit Estimates for Partial Invariance Loadings for Large Intercept Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}1 & 0.785 & 0.097\\
lv \textasciitilde{}1 & 0.785 & 0.097\\
q2 \textasciitilde{}1 & 0.781 & 0.100\\
q3 \textasciitilde{}1 & 0.781 & 0.100\\
q4 \textasciitilde{}1 & 0.988 & 0.023\\
q5 \textasciitilde{}1 & 0.784 & 0.099\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab18}Fit Estimates for Partial Invariance Loadings for Small Residual Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}\textasciitilde{} q1 & 0.928 & 0.056\\
q2 \textasciitilde{}\textasciitilde{} q2 & 0.936 & 0.053\\
q3 \textasciitilde{}\textasciitilde{} q3 & 0.926 & 0.057\\
q4 \textasciitilde{}\textasciitilde{} q4 & 0.955 & 0.044\\
q5 \textasciitilde{}\textasciitilde{} q5 & 0.926 & 0.057\\
lv \textasciitilde{}\textasciitilde{} lv & 0.930 & 0.054\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab19}Fit Estimates for Partial Invariance Loadings for Medium Residual Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}\textasciitilde{} q1 & 0.869 & 0.075\\
q2 \textasciitilde{}\textasciitilde{} q2 & 0.860 & 0.078\\
q3 \textasciitilde{}\textasciitilde{} q3 & 0.870 & 0.075\\
q4 \textasciitilde{}\textasciitilde{} q4 & 0.994 & 0.016\\
q5 \textasciitilde{}\textasciitilde{} q5 & 0.862 & 0.077\\
lv \textasciitilde{}\textasciitilde{} lv & 0.864 & 0.075\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:p-tab20}Fit Estimates for Partial Invariance Loadings for Large Residual Data}

\begin{tabular}{lll}
\toprule
Estimated Parameter & CFI & RSMEA\\
\midrule
q1 \textasciitilde{}\textasciitilde{} q1 & 0.558 & 0.140\\
q2 \textasciitilde{}\textasciitilde{} q2 & 0.559 & 0.140\\
q3 \textasciitilde{}\textasciitilde{} q3 & 0.560 & 0.140\\
q4 \textasciitilde{}\textasciitilde{} q4 & 0.972 & 0.035\\
q5 \textasciitilde{}\textasciitilde{} q5 & 0.559 & 0.140\\
lv \textasciitilde{}\textasciitilde{} lv & 0.562 & 0.137\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{invariance-plots-difference-scores-by-condition}{%
\section{Invariance Plots Difference Scores by Condition}\label{invariance-plots-difference-scores-by-condition}}

\includegraphics{manuscript_files/figure-latex/inv-plots-condition-1.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-2.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-3.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-4.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-5.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-6.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-7.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-8.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-9.pdf} \includegraphics{manuscript_files/figure-latex/inv-plots-condition-10.pdf}

\hypertarget{invariance-plots-effect-sizes-by-condition}{%
\section{Invariance Plots Effect Sizes by Condition}\label{invariance-plots-effect-sizes-by-condition}}

\includegraphics{manuscript_files/figure-latex/inv-effect-condition-1.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-2.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-3.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-4.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-5.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-6.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-7.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-8.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-9.pdf} \includegraphics{manuscript_files/figure-latex/inv-effect-condition-10.pdf}

\hypertarget{density-plots-by-condition}{%
\section{Density Plots by Condition}\label{density-plots-by-condition}}

\includegraphics{manuscript_files/figure-latex/density-effects-condition-1.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-2.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-3.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-4.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-5.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-6.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-7.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-8.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-9.pdf} \includegraphics{manuscript_files/figure-latex/density-effects-condition-10.pdf}

\hypertarget{replication-test}{%
\subsection{Replication Test}\label{replication-test}}

\hypertarget{data}{%
\subsection{Data}\label{data}}

\begin{verbatim}
## # A tibble: 2 x 2
##   group       sample
##   <chr>        <int>
## 1 Chen          1010
## 2 Schulenberg   1765
\end{verbatim}

\hypertarget{mgcfa}{%
\subsection{MGCFA}\label{mgcfa}}

\begin{verbatim}
## # A tibble: 7 x 7
##   model                 AIC     BIC   cfi   tli  rmsea   srmr
##   <chr>               <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl>
## 1 Overall           113442. 113608. 0.939 0.928 0.0890 0.0344
## 2 Group Schulenberg  69101.  69254. 0.928 0.915 0.108  0.0353
## 3 Group Chen         42603.  42741. 0.929 0.916 0.0766 0.0399
## 4 Configural        111760. 112258. 0.929 0.916 0.0975 0.0347
## 5 loadings          111789. 112210. 0.927 0.921 0.0946 0.0427
## 6 intercepts        112785. 113129. 0.892 0.891 0.111  0.0599
## 7 residuals         113318. 113579. 0.873 0.880 0.116  0.0661
\end{verbatim}

Overall, the one-factor model fits the data well. Each group also shows adequate model fit. If we use \(\Delta\)CFI \textless= .01, we find that the loadings would be considered invariant across the English and Chinese samples. The intercepts were not invariant.

\hypertarget{partial-invariance}{%
\subsection{Partial Invariance}\label{partial-invariance}}

\begin{verbatim}
## # A tibble: 15 x 2
##    free.parameter cfi       
##    <chr>          <lvn.vctr>
##  1 "RS2 ~1 "      0.8970997 
##  2 "RS11 ~1 "     0.8963948 
##  3 "RS4 ~1 "      0.8961938 
##  4 "RS12 ~1 "     0.8960095 
##  5 "RS14 ~1 "     0.8954496 
##  6 "RS1 ~1 "      0.8954215 
##  7 "RS10 ~1 "     0.8953395 
##  8 "RS3 ~1 "      0.8948455 
##  9 "RS5 ~1 "      0.8945236 
## 10 "RS7 ~1 "      0.8941450 
## 11 "RS8 ~1 "      0.8920022 
## 12 "RS13 ~1 "     0.8919491 
## 13 "RS9 ~1 "      0.8919293 
## 14 "RS ~1 "       0.8917757 
## 15 "RS6 ~1 "      0.8917604
\end{verbatim}

Examining partial invariance reveals several potential candidates for partial invariance. In this next section, we relaxed group constraints until we achieved partial invariance (i.e., \(\Delta\)CFI \textless= .01). We will need to find our CFI as at least 0.92. More than half the items are necessary to achieve ``partial'' invariance (which really implies no invariance is likely possible).

\begin{verbatim}
## # A tibble: 1 x 6
##       AIC     BIC   cfi   tli  rmsea   srmr
##     <dbl>   <dbl> <dbl> <dbl>  <dbl>  <dbl>
## 1 111935. 112326. 0.922 0.917 0.0966 0.0462
\end{verbatim}

\begin{verbatim}
## # A tibble: 6 x 3
##   term       English Chinese
##   <chr>        <dbl>   <dbl>
## 1 "RS1 ~1 "     5.24    4.78
## 2 "RS3 ~1 "     5.14    5.46
## 3 "RS4 ~1 "     5.29    5.71
## 4 "RS5 ~1 "     5.13    5.13
## 5 "RS7 ~1 "     5.26    5.26
## 6 "RS12 ~1 "    5.57    5.16
\end{verbatim}

\begin{verbatim}
##  RS1  RS2  RS3  RS4  RS5  RS6  RS7  RS8  RS9 RS10 RS11 RS12 RS13 RS14 
## 0.32 0.29 0.23 0.30 0.00 0.00 0.00 0.00 0.00 0.29 0.27 0.29 0.00 0.26
\end{verbatim}

\hypertarget{visualize-invariance}{%
\subsection{Visualize Invariance}\label{visualize-invariance}}

\begin{verbatim}
## Plot for RS2
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-1.pdf}

\begin{verbatim}
## -------
## Plot for RS11
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-2.pdf}

\begin{verbatim}
## -------
## Plot for RS4
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-3.pdf}

\begin{verbatim}
## -------
## Plot for RS12
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-4.pdf}

\begin{verbatim}
## -------
## Plot for RS14
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-5.pdf}

\begin{verbatim}
## -------
## Plot for RS1
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-6.pdf}

\begin{verbatim}
## -------
## Plot for RS10
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-7.pdf}

\begin{verbatim}
## -------
## Plot for RS3
\end{verbatim}

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-103-8.pdf}

\begin{verbatim}
## -------
\end{verbatim}

\hypertarget{bootstrap-model}{%
\subsection{Bootstrap Model}\label{bootstrap-model}}

\begin{verbatim}
##        model non_invariant random_non_invariant    h_nmi h_nmi_p
## 1 intercepts             1                    0 3.141593       1
\end{verbatim}

In this case, we do not see loadings print out. That implies that all models in both real data and randomized data are invariant because the function only calculates information for non-invariance. We see that the intercepts are unlikely to ever replicate across Chinese and English samples. This result is not surprising given the large number of relaxed parameters required to achieve partial invariance.

\hypertarget{bootstrap-partial-invariance}{%
\subsection{Bootstrap Partial Invariance}\label{bootstrap-partial-invariance}}

\hypertarget{each-parameter-on-the-overall-model-invariance}{%
\subsubsection{Each Parameter on the Overall Model Invariance}\label{each-parameter-on-the-overall-model-invariance}}

\begin{verbatim}
##        term non_invariant random_non_invariant    h_nmi  h_nmi_p
## 1    RS ~1              1                0.002 3.052120 0.971520
## 2   RS1 ~1              1                0.001 3.078337 0.979865
## 3  RS10 ~1              1                0.002 3.052120 0.971520
## 4  RS11 ~1              1                0.002 3.052120 0.971520
## 5  RS12 ~1              1                0.002 3.052120 0.971520
## 6  RS13 ~1              1                0.002 3.052120 0.971520
## 7  RS14 ~1              1                0.002 3.052120 0.971520
## 8   RS2 ~1              1                0.002 3.052120 0.971520
## 9   RS3 ~1              1                0.002 3.052120 0.971520
## 10  RS4 ~1              1                0.002 3.052120 0.971520
## 11  RS5 ~1              1                0.002 3.052120 0.971520
## 12  RS6 ~1              1                0.002 3.052120 0.971520
## 13  RS7 ~1              1                0.001 3.078337 0.979865
## 14  RS8 ~1              1                0.002 3.052120 0.971520
## 15  RS9 ~1              1                0.002 3.052120 0.971520
\end{verbatim}

In this output, we see that all the bootstrapped runs of the real data are non-invariant, even when each parameter is relaxed individually. A few runs of the random data are \emph{non}-invariant (meaning most are actually invariant when randomized). This indicates that no one parameter is likely the reason for non-invariance, as they all show large non-replication effects. If we use the \texttt{boot\_summary}, we can see the effect size for each parameter when the two intercepts are compared to each other (as the chart above shows the overall model invariance effect).

\hypertarget{each-parameters-standardized-difference-score}{%
\subsubsection{Each Parameter's Standardized Difference Score}\label{each-parameters-standardized-difference-score}}

\begin{verbatim}
##        term invariant n_boot     d_boot
## 1    RS ~1      FALSE   1000         NA
## 2   RS1 ~1      FALSE   1000  0.2913872
## 3  RS10 ~1      FALSE   1000  0.3596961
## 4  RS11 ~1      FALSE   1000 -0.6589317
## 5  RS12 ~1      FALSE   1000 -0.3010202
## 6  RS13 ~1      FALSE   1000 -0.4005088
## 7  RS14 ~1      FALSE   1000 -0.5665988
## 8   RS2 ~1      FALSE   1000 -0.5540349
## 9   RS3 ~1      FALSE   1000 -0.6218628
## 10  RS4 ~1      FALSE   1000 -0.6245830
## 11  RS5 ~1      FALSE   1000 -0.3334520
## 12  RS6 ~1      FALSE   1000 -0.5014247
## 13  RS7 ~1      FALSE   1000 -0.2381644
## 14  RS8 ~1      FALSE   1000 -0.6333750
## 15  RS9 ~1      FALSE   1000 -0.5986866
\end{verbatim}

\begin{verbatim}
##        term invariant n_random d_random
## 1    RS ~1      FALSE        2       NA
## 2   RS1 ~1      FALSE        1       NA
## 3  RS10 ~1      FALSE        2       NA
## 4  RS11 ~1      FALSE        2       NA
## 5  RS12 ~1      FALSE        2       NA
## 6  RS13 ~1      FALSE        2       NA
## 7  RS14 ~1      FALSE        2       NA
## 8   RS2 ~1      FALSE        2       NA
## 9   RS3 ~1      FALSE        2       NA
## 10  RS4 ~1      FALSE        2       NA
## 11  RS5 ~1      FALSE        2       NA
## 12  RS6 ~1      FALSE        2       NA
## 13  RS7 ~1      FALSE        1       NA
## 14  RS8 ~1      FALSE        2       NA
## 15  RS9 ~1      FALSE        2       NA
\end{verbatim}

With the bootstrap summary, we see that no invariant intercept runs are found (not all columns shown to save space). Note that the \texttt{RS\textasciitilde{}1} does not calculate \emph{d} because the \emph{sd} is zero (thus, this would cause an error as \emph{sd} is part of the denominator). The random runs also do not show a \emph{d} score because the effect size is only calculated when at least 10\% of the runs in \texttt{n\_boot} or \texttt{n\_random} are found. Since we only have a few in that column, no effect size is calculated. The differences in the intercepts for the real data (\texttt{boot} columns) appear to be medium to large, showing \emph{d} scores from 0.3 to 0.6. Therefore, we might expect that the English and Chinese samples have different average endorsement levels of the RS14.

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-110-1.pdf}

The examination of the density plot shows how group 1 (Schulenberg English) tends to show lower average scores than group 2 (Chen Chinese) for most but not all items.

We can view the mean difference or standardized mean difference by using:

\includegraphics{manuscript_files/figure-latex/unnamed-chunk-111-1.pdf} \includegraphics{manuscript_files/figure-latex/unnamed-chunk-111-2.pdf}

When effects are non-invariant in the randomized data, the mean difference is still fairly small, but we see large mean differences in intercepts when the bootstrapped data is non-invariant. In the effect size graph, we can see that this effect is medium to large for all the parameters.


\end{document}
